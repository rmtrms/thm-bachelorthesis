\chapter{Benchmark und Modellauswahl}\label{ch:benchmark}

Die systematische Auswahl eines geeigneten \glspl{llm} erforderte die Entwicklung und Anwendung eines spezifisch auf den Anwendungsfall zugeschnittenen Benchmarks.
Die Konzeption und Durchführung war ein iterativer Prozess bei dem stetig Erkenntnisse und identifizierte Probleme dazu genutzt wurden, die Rahmenbedingungen des Benchmarks zu verbessern.
Dieser iterative Prozess hatte zur Folge, dass aufbauend auf den Zwischenergebnissen Erkenntnisse gewonnen und entsprechende Anpassungen vorgenommen wurden.
Zur Nachvollziehbarkeit der Entstehung des Benchmarks wird in den folgenden Unterkapiteln nicht nur auf das finale Ergebnis, sondern ebenso auf die Zwischenergebnisse und die daraus abgeleiteten Entscheidungen eingegangen.

% ======================================================================================================================

\section{Konzeption}\label{sec:konzeption-benchmark}

% TODO: konkret die reduzierte Policy ansprechen
Ziel des Benchmarks ist es, ein geeignetes \gls{llm} zu bestimmen.
Es wird geprüft, inwiefern jedes \gls{llm} Copyright-Informationen extrahieren kann, die Policy-konformen Erwartungshaltungen entsprechen.
Hierzu wird das \gls{llm} mit einem Prompt dazu aufgefordert, die Copyright-Statements, Urheber und Autoren für eine bestimmte Eingangsdatei zu bestimmen und anschließend wird die Antwort des \gls{llm} mit einer vorher formulierten Erwartungshaltung abgeglichen.
Dieser Schritt wird für jede Datei des Benchmark-Datensatzes durchgeführt.
Der verwendete Datensatz wird im Abschnitt \nameref{sec:datensatz-benchmark} thematisiert.
Schließlich die Ergebnisse des \gls{llm} anhand mehrerer Metriken ausgewertet und mit den Ergebnissen anderer \glspl{llm} verglichen.

\subsection{Lokale Ausführung der LLMs}\label{subsec:lokale-ausfuehrung}

Um konsistente Ergebnisse über mehrere Durchläufe hinweg zu erzielen, muss sichergestellt werden, dass die Ausgaben der Modelle für denselben Prompt bei mehrmaliger Durchführung nicht variieren.
Hierzu wird die Temperatur der \glspl{llm} auf 0,0 gestellt.
Diese Einstellung ermöglicht es, die Varianz zwischen den Ausgaben, welche bei kreativer Textgenerierung durchaus erwünscht ist, zu eliminieren und sorgt somit für eine deterministische Ausgabe.
Die Funktionalität der Temperatur wurde praktisch geprüft, indem ein Modell in einer bestimmten Größe und Version auf zwei verschiedenen Endgeräten mit demselben Prompt angewiesen wurde und in beiden Fällen eine identische Ausgabe generierte.
Um sicherzustellen, dass im Verlauf des Benchmarks und darüber hinaus keine Updates der Modelle die Ergebnisse verfälschen, müssen feste Versionen der \glspl{llm} verwendet werden.
Da proprietäre \glspl{llm} wie ChatGPT oder Gemini keine Steuerung der Versionen oder Temperatur ermöglichen, ist eine lokale Ausführung von Open-Source-Modellen erforderlich.
Außerdem fallen für die Nutzung der \gls{api} dieser Modelle Kosten an, weshalb eine lokale Ausführung auch hier zu bevorzugen ist.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Auswahl der Sprachmodelle}\label{sec:modelle-benchmark}

Die Auswahl an verfügbaren \glspl{llm} wird einerseits durch die Leistungsfähigkeit der verwendeten Hardware und andererseits durch die Kompatibilität der Software eingeschränkt.
Erste Experimente ergaben, dass der verwendete Mac Mini M4 Pro mit 64 Gigabyte Arbeitsspeicher \glspl{llm} mit einer Parametergröße von maximal 108 Milliarden ausführen kann, ohne wärmebedingt die Leistung zu drosseln.
Da die finale Implementierung des Benchmarks softwareseitig mit Ollama4J einen lokal ausgeführten Ollama Server anspricht, konnten nur solche \glspl{llm} ausgewählt werden, die auf Ollama zur Verfügung stehen.
Die Wahl an \glspl{llm}, die für den Benchmark genutzt werden sollen, beschränkt sich somit auf Modelle die durch Ollama unterstützt werden und bis zu 108 Milliarden Parameter groß sind.

Es wurden \glspl{llm} von zahlreichen namhaften Providern geprüft, darunter Microsoft (phi4, phi3), Meta (llama4, llama3), Deepseek (deepseek-coder, deepseek-r1), Alibaba (qwen3, qwen2.5, qwen2.5-coder), Google (gemma3, gemma3n) und Mistral (mistral, mathstral, devstral, mistral-small, mistral-nemo).
Zusätzlich zu etablierten Modellen wurden auch gezielt exotischere und weniger verbreitete Modelle ausgewählt (llava, olmo2, orca-mini, dolphin).

Neben den Providern spielte auch die ausgeschriebene Verwendung der Modelle eine Rolle, somit wurden Modelle für verschiedene Anforderungen wie Vision (llava), Reasoning (qwen3, mixstral) und Programmierung (devstral, qwen2.5-coder) ausgewählt.

Um einen Vergleich in Hinsicht auf Präzision und Performance anhand der Modellgröße zu ermöglichen, wurden gezielt kleine Modelle (tinyllama:1.1b, orca-mini:3b, qwen2.5-coder:0.5b), mittelgroße Modelle (mistral-small:24b, qwen2.5-coder:32b) und große Modelle (llama4:108.6B, mixstral:8x7b) verglichen.

Die finale Durchführung des Benchmarks umfasste 30 \glspl{llm}.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Zusammenstellung des Datensatzes}\label{sec:datensatz-benchmark}

Die Ergebnisse der Datenaggregation wurden dazu genutzt, einen kuratierten Benchmark-Datensatz zu erstellen.
Hierzu wurden repräsentative Fälle aus der Datenaggregation identifiziert und ihre Erwartungshaltung anhand der Policy überprüft und wenn nötig, angepasst.
Der Datensatz umfasst insgesamt \num{200} Dateien und ihre überprüften Erwartungshaltungen.
Die \num{200} Dateien sind in acht Kategorien mit jeweils \num{25} Dateien unterteilt:

\begin{itemize}
    \item single copyrights with authors
    \item single copyrights without authors
    \item multiple copyrights with authors
    \item multiple copyrights without authors
    \item case-insensitive matches
    \item format-insensitive matches
    \item only authors
    \item no copyrights or authors or holders
\end{itemize}

Die ersten vier Kategorien stellen die \enquote{normalen} Fälle dar, wobei die \textit{case-insensitive} und \textit{format-insensitive} Fälle speziell die Normalisierungen des ScanCode-Toolkits abdecken.
Die Kategorien \textit{no copyrights or authors or holders} und \textit{only authors} dienen gezielt dazu, das Verhalten des Modells bei Abwesenheit von Copyright-Statements zu untersuchen sowie die alleinige Autorenextraktion.

Um repräsentative Fälle einer Kategorie zusammenzustellen wurden je \num{25} Dateien aus den Hauptkategorien des Datensatzes zufällig ausgewählt und anschließend gesichtet.
Ähnliche Fälle (z.B.\ mehrere Copyright-Statements desselben Urhebers) wurden identifiziert und entsprechend ersetzt.
Somit konnte sichergestellt werden, dass kein Copyright, Urheber oder Autor überrepräsentiert im Datensatz vorkommt.

Die relativ kleine Anzahl von Dateien wurde dadurch kompensiert, dass mithilfe der Kategorien möglichst unterschiedliche Arten von Statements und Autoren im Benchmark enthalten sind.
Darüber hinaus ist jede Kategorie mit derselben Anzahl an Dateien vertreten, wodurch eine Bevorzugung bzw.\ Benachteiligung eines Modells anhand ungleichmäßiger Verteilung von Fällen zusätzlich mitigiert wird.

% TODO: konkret die reduzierte Policy ansprechen
Die Formulierung der Erwartungshaltungen der ausgewählten Dateien wurde anhand einer reduzierten Policy durchgeführt.
Die reduzierte Policy enthielt lediglich die \nameref{subsec:cir-01}, sowie die \nameref{subsec:cep-01} und \nameref{subsec:cep-02}, wobei die \nameref{subsec:hir-01} und \nameref{subsec:air-01} implizit umgesetzt wurden, da sie zu diesem Zeitpunkt noch nicht ausreichend formuliert waren.

% ======================================================================================================================

\section{Implementierung}\label{sec:benchmark-implementierung}

Für die Umsetzung des Benchmarks wurde ein Java-basiertes Backend entwickelt, das über die Bibliothek Ollama4J\footnote{\url{https://github.com/ollama4j/ollama4j}} mit einem lokal ausgeführten \gls{llm} kommuniziert.
Ollama4J stellt Java-Bindings für Ollama\footnote{\url{https://ollama.com}} bereit, eine leichtgewichtige Laufzeitumgebung für \glspl{llm}, die über eine standardisierte REST-API angesteuert wird.
Die Verwendung von Ollama ermöglichte eine einfache Modellanbindung sowie ein benutzerfreundliches Modellmanagement.
Eine Alternative zu Ollama stellt llama.cpp\footnote{\url{https://github.com/ggml-org/llama.cpp}} dar.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Auswahl von Ollama}\label{subsec:auswahl-von-ollama}

Ollama und llama.cpp verfolgen ähnliche Ziele, unterscheiden sich jedoch in ihrer Ausrichtung.
Während llama.cpp eine performante, in C++ implementierte Bibliothek zur Ausführung von \glspl{llm} darstellt und typischerweise direkt in Host-Anwendungen eingebunden wird, ist Ollama als eigenständige Laufzeitumgebung konzipiert.
Über eine REST-API ermöglicht Ollama den standardisierten Zugriff auf lokal ausgeführte Modelle und stellt zudem Werkzeuge zur komfortablen Verwaltung und zum Austausch von Modellen bereit.

Für die vorliegende Arbeit fiel die Wahl auf Ollama, da es durch seine integrierte Modellverwaltung den schnellen Wechsel zwischen verschiedenen \glspl{llm} deutlich vereinfacht.
Gerade bei Benchmarks, in denen zahlreiche Modelle getestet und verglichen werden, reduziert dies den organisatorischen Aufwand erheblich.
Darüber hinaus gestaltet sich das Setup von Ollama unkompliziert, sodass Modelle ohne tiefgreifende Konfiguration lauffähig gemacht werden können.

Ein weiterer Vorteil ergibt sich aus der Trennung der Verantwortlichkeiten: Ollama wurde in einem Docker-Container ausgeführt, wodurch eine klare Separation of Concerns erreicht wurde.
Das Benchmark-Backend konnte dadurch unabhängig vom Modell-Backend betrieben werden.
Im Gegensatz dazu läuft llama.cpp typischerweise als Teil des Java-Prozesses und bietet damit eine geringere Isolation zwischen Anwendung und Modell.
Ollama erwies sich somit als flexiblere und wartungsfreundlichere Lösung für die Durchführung der Benchmark-Experimente.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{LLM Warm-Up}\label{subsec:llm-warm-up}

Da Ollama beim erstmaligen Ausführen eines \glspl{llm} das Modell zunächst in den Arbeitsspeicher lädt, entsteht eine initiale Verzögerung vor Beginn der \gls{glos:inferenz}.
Diese Verzögerung tritt zu Beginn des \gls{glos:benchmark} auf und verfälscht die Messung der Laufzeit beim ersten Prompt.
Um die Metrik Tokens/sec unabhängig von diesem Ladevorgang zu bestimmen, wurde ein Warm-Up-Verfahren implementiert.
Dabei wird das Modell vor dem eigentlichen \gls{glos:benchmark} einmalig aufgefordert, mit einem einzelnen Wort (\enquote{ready}) zu antworten.
Sobald diese Antwort vorliegt, gilt das \gls{llm} als geladen und einsatzbereit, sodass im Anschluss die \gls{glos:benchmark}-Messung ohne zusätzliche Initialisierungszeit durchgeführt werden kann.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Zeitüberschreitung der Anfragen}\label{subsec:zeituberschreitung-der-anfragen}

Bei den Untersuchungen zeigte sich insbesondere bei Reasoning-Modellen, dass diese bei komplexeren Instruktionen in Endlosschleifen geraten und dadurch keine vollständige Antwort abschließen.
Zwar bietet Ollama4J die Möglichkeit, einen Timeout zu definieren, dieser greift jedoch primär in Situationen, in denen ein Modell nicht korrekt geladen wird und folglich keine Antwort liefert.
Die hier beobachtete Problematik betrifft hingegen nicht das Ausbleiben, sondern die unerwünscht lange Generierung einer Antwort.

Um diesem Verhalten zu begegnen, wurde ein zusätzlicher Mechanismus implementiert, der die \gls{glos:inferenz} abbricht, falls ein Modell seine Antwort nicht innerhalb einer vorgegebenen Zeitspanne beendet.
Jede Anfrage an Ollama wird hierzu in einem \textit{Single-Thread-Executor-Service} ausgeführt, der die jeweilige Anfrage nach drei Minuten beendet und einen entsprechenden Timeout vermerkt.
Die Ausführung über den Executor-Service ermöglicht eine präzise Kontrolle des Prozesses auf Anwendungsebene, ohne sich auf die interne Steuerung durch Ollama verlassen zu müssen.

Für die Auswertung des \gls{glos:benchmark}s wird zudem die Anzahl der Anfragen mit Zeitüberschreitung gesondert erfasst.
Die Berechnung der Laufzeit pro Anfrage berücksichtigt ausschließlich erfolgreiche Anfragen, während Anfragen mit Timeout von der Laufzeitmetrik ausgeschlossen werden.
Auf diese Weise fließen nur valide Messwerte in die \gls{glos:benchmark}-Ergebnisse ein.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Vorverarbeitung der Eingabedatei}\label{subsec:vorverarbeitung-der-eingabedatei}

\glspl{llm} verfügen über unterschiedliche Kontextgrößen, die bestimmen, wie viele Tokens ein Modell gleichzeitig verarbeiten kann, ohne Informationen zu verlieren oder fehlerhafte Antworten zu erzeugen.
Während große Modelle wie LLama4 Scout\footnote{\url{https://ai.meta.com/blog/llama-4-multimodal-intelligence/}} eine Kontextgröße von bis zu 10 Millionen Tokens unterstützen und somit ganze Bücher verarbeiten können, sind kleinere Modelle wie Mistral:7B\footnote{\url{https://mistral.ai/news/announcing-mistral-7b}} auf lediglich 32.800 Tokens beschränkt.
Die Größe des Kontextfensters hat unmittelbare Auswirkungen sowohl auf die Präzision als auch auf die Laufzeit.
Ein Überschreiten der maximalen Kontextgröße kann die Qualität der Modellantworten erheblich beeinträchtigen\autocite{zhang_sinklora_2024}.

Der in Kapitel~\ref{ch:daten} beschriebene Trainingsdatensatz enthält unter anderem Dateien mit mehreren tausend Zeilen Quellcode, von denen jedoch nur ein geringer Teil für die Extraktion des Copyrights relevant ist.
In den Experimenten zeigte sich, dass das Überschreiten der Kontextgröße bei lokal ausgeführten \glspl{llm} regelmäßig dazu führte, dass keine verwertbaren Ergebnisse mehr generiert wurden.
Zudem neigten Modelle bei hohem Quellcode-Anteil dazu, die Aufgabe fehlzuinterpretieren, indem sie statt einer Extraktion neue Codefragmente generierten, die eine hypothetische Lösung darstellen sollten.

Da im Benchmark verschiedene Modelle mit stark variierenden Kontextgrößen eingesetzt wurden, war eine Vereinheitlichung notwendig.
Um vergleichbare Bedingungen zu schaffen, wurde das Kontextfenster für alle Modelle auf \num{4096} Tokens begrenzt.
Diese Begrenzung entspricht dem Standardwert von Ollama, sofern keine explizite Vergrößerung vorgenommen wird.

Die Kombination aus unterschiedlichen Kontextgrößen und sehr großen Eingabedateien machte eine Vorverarbeitung erforderlich, um die Eingaben auf copyright-relevante Inhalte zu reduzieren.
Hierfür wurde eine metaeffekt-interne Java-Klasse eingesetzt, die auf Basis einer bereitgestellten Wortliste Filterungen vornimmt.
Die Dateien werden dabei nach den Begriffen durchsucht wobei jedes Vorkommen eine Maske setzt, die alle Zeichen im definierten Umgebungsbereich beibehält, während irrelevante Teile entfernt werden.
Auf diese Weise konnte mithilfe einer aggregierten und sukzessive optimierten Wortliste sichergestellt werden, dass nur relevante Textabschnitte an das \gls{llm} übergeben werden.

Zur Validierung wurden die gefilterten Dateien gemeinsam mit den Originaldateien zwischengespeichert, um nachträglich prüfen zu können, ob keine für die Extraktion erforderlichen Informationen verloren gegangen sind.
Durch diese Vorverarbeitung ließen sich Eingabedateien mit mehr als \num{100000} Tokens auf unter \num{4096} Tokens reduzieren, wodurch eine konsistente und modellübergreifende Verarbeitung im Benchmark gewährleistet wurde.

Darüber hinaus wird für jede Anfrage die Anzahl der Tokens des vollständigen Prompts, bestehend aus Aufgabenstellung und gefilterter Eingabedatei, berechnet und in der Benchmark-Auswertung erfasst.
Zusätzlich werden die minimale und maximale Tokenzahl über alle Anfragen dokumentiert.
Dadurch kann überprüft werden, dass auch der längste Prompt innerhalb des vorgegebenen Kontextfensters von \num{4096} Tokens liegt.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Strukturierte Ausgabe}\label{subsec:strukturierte-ausgabe}

Damit die Ergebnisse der Extraktion für nachgelagerte automatische Prozesse nutzbar sind, müssen sie in einem strukturierten Format vorliegen.
Als Zielformat wurde ein \gls{json}-Schema gewählt, das dem des ScanCode-Toolkits entspricht.
Dies gewährleistet einerseits die Vergleichbarkeit der Ergebnisse und erleichtert andererseits die Integration in bestehende Prozessketten.

Damit ein \gls{llm} die Ausgabe in einem solchen Schema erzeugt, muss es im Prompt explizit instruiert werden, ausschließlich ein valides \gls{json} ohne zusätzliche Inhalte zu generieren.
In der Praxis zeigte sich jedoch, dass viele Modelle trotz entsprechender Vorgabe die Ergebnisse mit zusätzlichem Text versehen, etwa durch Einbettung in Markdown oder durch einleitende Beschreibungen wie \enquote{Here is the extracted JSON:}.
Auch fehlerhafte oder unvollständige \gls{json}-Strukturen traten vereinzelt auf.

Um diese Abweichungen zu beheben, wurde ein Mechanismus zur Validierung und Korrektur der Modellantworten integriert.
Dieser versucht, die generierte Ausgabe in ein gültiges \gls{json} zu parsen und löst im Fehlerfall eine entsprechende Fehlermeldung aus.
Eine konkrete Implementierung hierfür stellt die von Yan Wittmann entwickelte Klasse \textit{ChatUtil}\footnote{\url{https://github.com/YanWittmann/automatic-document-classification/blob/main/src/main/java/de/yanwittmann/document/ai/ChatUtil.java}} bereit, welche Antworten von \glspl{llm} zuverlässig in gültige \gls{json}-Strukturen überführt.
Durch die Verwendung dieser Klasse können unerwünschte Formatierungen oder Zusatzinformationen entfernt und konsistente, maschinenlesbare Ausgaben gewährleistet werden.

Im Rahmen der Ergebnisauswertung wird darüber hinaus erfasst, wie viele Modellantworten ungültige \gls{json}-Strukturen enthalten haben.
Zusätzlich wird dokumentiert, in wie vielen Fällen diese fehlerhaften Ausgaben mithilfe von \textit{ChatUtil} erfolgreich in gültige \gls{json}s überführt werden konnten.
Dadurch lässt sich nachvollziehen, in welchem Maße die Modelle die strukturelle Konsistenz ihrer Ausgaben sicherstellen und wie stark die nachgelagerte Korrektur zur Ergebnisqualität beiträgt.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Ergebnisauswertung}\label{subsec:ergebnisauswertung}

Um die Leistungsfähigkeit der untersuchten \glspl{llm} systematisch vergleichen und detailliert analysieren zu können, werden bei jedem Durchlauf verschiedene Metriken und Kennzahlen aufgezeichnet.
Die Ergebnisse werden in einem strukturierten Datenformat gesammelt und als JSON-Datei gespeichert.

Die Auswertung umfasst sowohl aggregierte Metriken über den gesamten Datensatz hinweg als auch differenzierte Ergebnisse für einzelne Kategorien und spezifische Dateien.
Auf diese Weise lässt sich nachvollziehen, wie jedes Modell mit unterschiedlichen Eingabearten und individuellen Dateien umgegangen ist.
Das Ergebnis ist eine umfassende Analysedatei, die sowohl für manuelle Betrachtungen als auch für eine automatisierte Weiterverarbeitung geeignet ist.

% ======================================================================================================================

\section{Metriken}\label{sec:metriken-benchmark}

Eine systematische Analyse der Eignung mehrerer \glspl{llm} erfordert mehrere Metriken, die zur Evaluation herangezogen werden.
Die in diesem Benchmark verwendeten Metriken werden nachfolgend erläutert, dabei werden die generierten Ergebnisse des \gls{llm} wie folgt interpretiert:
\begin{itemize}
    \item \textit{\gls{tp}} -- ein Element aus der Erwartungshaltung wird korrekt extrahiert (entweder exakte oder ausreichend ähnliche Übereinstimmung). Sofern ein \gls{tp} für ein Element aus der Erwartungshaltung gefunden wurde, wird dieses Element für die weitere Bewertung ausgeschlossen da jedes Element nur einmalig zugewiesen werden kann.
    \item \textit{\gls{fp}} -- ein vom Modell extrahiertes Element ist nicht in der Erwartungshaltung enthalten oder es ist verbleibt kein ausreichend ähnliches Element.
    \item \textit{\gls{fn}} -- ein Element aus der Erwartungshaltung wird vom Modell nicht extrahiert (weder exakte noch ausreichend ähnliche Übereinstimmung).
\end{itemize}

\subsection{Exact Matches}
Die zentrale Metrik zur Bewertung der Extraktion von Copyright-Statements ist der Anteil sogenannter \textit{Exact Matches}.
Ein \textit{Exact Match} liegt vor, wenn ein vom \gls{llm} generiertes Statement exakt mit einem Statement der vorher formulierten Erwartungshaltung übereinstimmt.
Durch eine perfekte Extraktion ohne zusätzliche Zeichen oder fehlerhafte Normalisierungen wird die \nameref{subsec:cep-01} erfüllt.
Mehrere Studien nutzen \textit{Exact Matches} als Bewertungsmaß für die Leistung von \gls{ner}- und \gls{ie}-Systemen auf Basis von \glspl{llm} \autocite{dunn_structured_2022}\autocite{hu_improving_2024}.

\subsection{F1-Score}
Eine gängige Metrik zur Bewertung von Klassifikationsmodellen ist der F1-Score\autocite{noauthor_f-score_2025}.
Der F1-Score stellt das harmonische Mittel aus Präzision (\textit{P}) und Recall (\textit{R}) dar.
Die Formeln für \textit{P}, \textit{R}, und \textit{F1} lauten wie folgt:

\[
 P = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}
\]

\[
 R = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}
\]

\[
 F1 = 2 \times \frac{P \times R}{P + R}
\]

Außerdem wird ein kombinierter F1-Score aller Elemente einer Eingabe $\mathrm{F1}_{overall}$ berechnet:

\[
 \mathrm{F1}_{overall} = \frac{\mathrm{F1}_{copyrights} + \mathrm{F1}_{holders} + \mathrm{F1}_{authors}}{3}
\]

In diesem Benchmark wird für jeden Entitätstyp der jeweilige F1-Score berechnet und zusätzlich ein durchschnittlicher F1-Score über alle Typen hinweg erfasst.
Zunächst wird jedes extrahierte Copyright-Statement auf einen \textit{Exact Match} mit der Erwartungshaltung geprüft.
Liegt dieser vor, wird das Statement als \gls{tp} gezählt.
Sofern kein \textit{Exact Match} vorliegt, wird die Jaro-Winkler-Ähnlichkeit\autocite{noauthor_jarowinkler_nodate} verwendet um das ähnlichste Element aus der Erwartungshaltung welches einen Similarity-Threshold vom \num{0,95} überschreitet, zu finden.
Wenn eine solche Übereinstimmung gefunden wird, zählt diese als \gls{tp}.
Bei Holders und Authors wird ausschließlich durch die Jaro-Winkler-Ähnlichkeit auf ähnliche Elemente in der Erwartungshaltung geprüft und entsprechende Fälle werden als \gls{tp} gezählt.

Diese Vorgehensweise bringt ein inhärentes Fehlermaß mit sich.
Elemente, die sich formal unterscheiden, etwa durch abweichende Schreibweisen, zusätzliche Leerzeichen oder kleinere Tippfehler, werden dennoch als korrekt gezählt, obwohl sie nicht vollständig übereinstimmen.
Gleichzeitig besteht das Risiko, dass semantisch relevante Unterschiede oberhalb des Schwellenwertes als korrekte Extraktion gewertet werden, obwohl sie inhaltlich nicht identisch sind.

Für die Bewertung der Ergebnisse ist dieses Fehlermaß jedoch vertretbar.
Die exakte Reproduktion der Copyright-Statements bleibt die wichtigste Grundlage für die Bewertung und wird über die Metrik der Exact Matches erfasst.
Bei den Kategorien Holders und Authors ist eine geringfügige Abweichung in der Schreibweise dagegen weniger kritisch, da der entscheidende Aspekt in der korrekten Identifikation der betreffenden Person oder Institution liegt.
Abweichungen wie unterschiedliche Namensvarianten oder alternative Schreibweisen beeinflussen die praktische Nutzbarkeit kaum.
Durch den bewusst hoch gewählten Threshold von \num{0,95} wird sichergestellt, dass nur sehr ähnliche Strings als \gls{tp} akzeptiert werden.
Auf diese Weise wird vermieden, dass größere Unterschiede als korrekt gezählt werden, und gleichzeitig bleibt die Bewertung realistisch.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Tokens/sec}
Zur zeitlichen Messung der Modellleistungen werden die \textit{Tokens/sec} berechnet.
Da Ollama4J keine Abfrage der tatsächlichen Token pro Sekunde eines Modells unterstützt muss eine Alternative herangezogen werden.
Um die Leistung der unterschiedlichen Modelle zu bestimmen, wird berechnet wie groß der Eingabe-Prompt ist und wie viel Zeit das Modell zur Bearbeitung benötigt.
Die Größe eines Prompts wird in Tokens gemessen, da die Modelle aber intern unterschiedliche Prozesse zur Tokenisierung eines Prompts verwenden, wird eine Annäherung verwendet.
Eine grobe Umrechnung von Eingabetext zu Tokens ist $1\;\text{Token}\approx 4\;\text{Buchstaben}$\autocite{noauthor_what_nodate}.
Die Dauer der Bearbeitung entspricht der Zeit zwischen dem Absenden des Eingabe-Prompts und der Beantwortung durch das Modell.
Die somit ermittelte Metrik stellt eine Vergleichbarkeit der Modell-Geschwindigkeit für alle \glspl{llm} dar, unabhängig von ihrer internen Tokenisierung.

% ======================================================================================================================

\section{Durchführung}\label{sec:durchfuhrung-benchmark}

In diesem Abschnitt wird die Durchführung des Benchmarks näher erläutert.
Grundlage bildet der in Abschnitt~\ref{subsec:eingabeprompt} beschriebene Eingabeprompt sowie ein zweistufiges Vorgehen bestehend aus einem initialen und einem finalen Durchlauf, wobei die im ersten Durchlauf gewonnenen Erkenntnisse in die finale Evaluation eingeflossen sind.

Die \autoref{fig:prompting_setup} veranschaulicht den Ablauf der Inferenz im Benchmark.
Zunächst wird die Eingabedatei durch ein Preprocessing auf copyright-relevante Inhalte reduziert.
Die resultierende reduzierte Eingabedatei wird anschließend zusammen mit dem Eingabeprompt an das \gls{llm} übergeben.
Auf dieser Basis generiert das Modell eine strukturierte Ausgabe im JSON-Format, die als Ergebnis der Extraktion weiterverarbeitet werden kann.

\begin{figure}[ht]
    \centering
    \makebox[\textwidth]{\includegraphics[width=1.3\textwidth]{benchmark/prompting}}
    \caption{Schematische Darstellung des Ablaufs der Inferenz. Zunächst wird die Eingabedatei mithilfe des Preprocessings auf relevante Inhalte reduziert und anschließend zusammen mit dem Eingabeprompt an das über Ollama ausgeführte LLM gesendet. Schließlich Antwortet das LLM mit den extrahierten Copyright-Informationen in einem strukturierten Format.}
    \label{fig:prompting_setup}
\end{figure}

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Eingabeprompt}\label{subsec:eingabeprompt}

% TODO: konkret die reduzierte Policy ansprechen
Der im Benchmark verwendete Eingabeprompt wurde mit dem Ziel entwickelt, die Modelle eindeutig und zuverlässig zur Extraktion von urheberrechtsrelevanten Informationen aus Texten anzuleiten.
Seine Struktur, sein Inhalt und seine Formatierung sind das Ergebnis iterativer Untersuchungen im Rahmen des \textit{Prompt-Engineering}, die in Kapitel~\ref{ch:prompt-engineering} detaillierter beschrieben werden.
Der vollständige Prompt ist im Anhang \ref{sec:anahng-eingabeprompt-benchmark} zu finden.

Der Prompt beginnt mit einer expliziten Rollenzuweisung, indem das \gls{llm} als Experte für die Extraktion von Copyright-Informationen adressiert wird.
Anschließend folgt eine klare Anweisung, dass die Ausgabe ausschließlich in Form eines gültigen \gls{json}-Objekts erfolgen darf, ohne begleitende Erklärungen oder zusätzliche Inhalte.
Diese strikte Formatvorgabe dient der Sicherstellung konsistenter und maschinenlesbarer Ergebnisse, die unmittelbar in nachgelagerten Prozessen weiterverarbeitet werden können.

\begin{lstlisting}[keepspaces=true]
You are an expert at extracting copyright information from text.
Your output MUST be a valid JSON object. Do NOT include any additional text, comments, or explanations outside the JSON.
\end{lstlisting}

Inhaltlich ist der Prompt so gestaltet, dass er drei klar abgegrenzte Zielkategorien definiert: \textit{copyrights}, \textit{holders} und \textit{authors}.
Für jede Kategorie wird präzisiert, welche Informationen aus dem Eingabetext zu extrahieren sind und in welcher Form diese in das \gls{json}-Schema einzutragen sind.
Dabei werden auch Grenzfälle berücksichtigt, etwa die Behandlung von \enquote{All Rights Reserved}-Vermerken oder die explizite Auslassung Lizenzinformationen.

\begin{lstlisting}[keepspaces=true]
Extract the following information from the provided text into a JSON object with these keys:
- "copyrights": A JSON array of strings. Each string must be an EXACT, verbatim copy of a copyright statement [...]
- "holders": A JSON array of strings. Each string must be the name of a copyright holder. [...]
- "authors": A JSON array of strings. Each string must be the name of an author mentioned in the context of copyright or authorship. [...]
\end{lstlisting}

Ein weiterer wichtiger Bestandteil des Prompts ist die Bereitstellung von Few-Shot-Beispielen, die sowohl Eingabetexte als auch die exakte Form der erwarteten Ausgabe zeigen.
Durch diese Demonstrationen wird das Modell mithilfe des \gls{icl} stärker in Richtung der gewünschten Struktur konditioniert und die Wahrscheinlichkeit von Abweichungen reduziert.

\begin{lstlisting}[keepspaces=true]
Example 1:
Text:
/*
Copyright (c) 2020 NVIDIA CORPORATION. All rights reserved.
Permission is hereby granted...
*/

Output:
{
  "copyrights": [ "Copyright (c) 2020 NVIDIA CORPORATION. All rights reserved." ],
  "holders": [ "NVIDIA CORPORATION" ],
  "authors": []
}
\end{lstlisting}

Am Ende des Prompts befindet sich ein Platzhalter, der während des Benchmarks durch den zu analysierenden Text ersetzt wird.
Damit bleibt der eigentliche Eingabeprompt inhaltlich konstant, während nur der relevante Dateiauszug dynamisch eingefügt wird.

\begin{lstlisting}[keepspaces=true]
Text to process:
{{FILE_CONTENT}}
\end{lstlisting}

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Zweistufige Durchführung}

Die Durchführung des Benchmarks erfolgte in zwei Stufen: einem initialen und einem finalen Durchlauf.
Nach mehreren vorbereitenden Testläufen, in denen verschiedene Probleme und Herausforderungen in der Implementierung identifiziert wurden, wurde der erste Benchmark mit einer Auswahl von \num{13} \glspl{llm} durchgeführt.
Eine Übersicht der initial eingesetzten Modelle einschließlich ihrer Parametergrößen und Quellen ist in Tabelle~\ref{tab:benchmark-models} im Anhang dargestellt.

Die Ergebnisse des ersten Durchlaufs zeigten, dass \texttt{mistral:7b} mit \num{80.69}\% den höchsten Prozentsatz an \textit{Exact Matches} generierte, während \texttt{qwen2.5-coder:7b} mit \num{0.84} den höchsten \textit{F1-Score} erreichte.
Diese Zwischenergebnisse deuteten darauf hin, dass insbesondere diese beiden Modelle besonders gut für die Aufgabe der Copyright-Extraktion geeignet sein könnten.

Um die Auswirkungen der Parametergröße auf Präzision und Laufzeit zu untersuchen, wurde der Benchmark im finalen Durchlauf um zusätzliche Modelle und Varianten erweitert.
Für \texttt{qwen2.5-coder} stehen auf Ollama mehrere Parametervarianten zur Verfügung, die vollständig in den Benchmark aufgenommen wurden.
Für \texttt{mistral:7b} existiert zwar nur eine einzelne Parametervariante, jedoch wurden mehrere eng verwandte Modelle der Untersuchung hinzugefügt.
Die ergänzten Modelle sind in Tabelle~\ref{tab:benchmark-models-extended} im Anhang aufgeführt.

Insgesamt wurden in der finalen Durchführung des Benchmarks \num{30} \glspl{llm} untersucht.

Die zugrunde liegenden Auswahlkriterien werden in Abschnitt~\ref{sec:modelle-benchmark} erläutert,
während Abschnitt~\ref{sec:ergebnisse-benchmark} eine detaillierte Analyse der Ergebnisse präsentiert.

% ======================================================================================================================

\section{Ergebnisse}\label{sec:ergebnisse-benchmark}

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Timeouts}

Einige der getesteten \glspl{llm} konnten nicht innerhalb des definierten Zeitlimits von drei Minuten eine vollständige Antwort generieren.
Die Tabelle~\ref{tab:model-timeouts} zeigt die Anzahl der aufgetretenen Timeouts pro Modell.
Modelle ohne Timeouts werden hier nicht aufgeführt.

\begin{table}[H]
    \centering
    \begin{tabular}{l c}
        \toprule
        \textbf{Modell} & \textbf{Timeouts} \\
        \midrule
        gemma3:4b            & 1  \\
        mathstral:7b         & 1  \\
        mixtral:8x7b         & 1  \\
        qwen2.5-coder:1.5b   & 1  \\
        qwen3:4b             & 19 \\
        qwen3:8b             & 62 \\
        tinyllama:1.1b       & 2  \\
        \bottomrule
    \end{tabular}
    \caption{Anzahl der Timeouts pro Modell}
    \label{tab:model-timeouts}
\end{table}

Besonders auffällig sind die Ergebnisse der beiden Varianten des Reasoning-Modells \texttt{qwen3}. Während \texttt{qwen3:4b} bereits 19 Timeouts verursachte, erreichte \texttt{qwen3:8b} mit 62 Timeouts den mit Abstand höchsten Wert aller getesteten Modelle.
Dieses Verhalten deutet darauf hin, dass die Reasoning-Funktion dieser Modellreihe in bestimmten Szenarien zu einer erheblich verlängerten oder gegebenenfalls nicht terminierenden Antwortgenerierung führen kann.

Darüber hinaus zeigten auch einige kleinere Modelle, wie \texttt{tinyllama:1.1b} oder \texttt{qwen2.5-coder:1.5b}, vereinzelte Timeouts.
Dies legt nahe, dass neben den speziellen Eigenschaften der Reasoning-Modelle auch begrenzte Modellkapazitäten bei kleineren Varianten zu Problemen bei der rechtzeitigen Antwortgenerierung beitragen können.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Wirksamkeit des Parsing-Mechanismus}

Durch die Integration der Klasse \texttt{ChatUtil} konnten Inhalte aus den Modellantworten zuverlässig herausgefiltert werden, die vor oder nach der eigentlichen JSON-Struktur standen.
In vielen Fällen betteten die Modelle ihre Ausgaben in Markdown ein oder fügten wiederkehrende, irrelevante Elemente hinzu, die jedoch erfolgreich entfernt werden konnten.

Besonders deutlich zeigte sich dieses Verhalten bei \texttt{mistral-small:24b} oder \texttt{gemma3n:e4b}, die nahezu alle Antworten in folgender Form zurückgaben:

\begin{lstlisting}[keepspaces=true]
```json
{
 "copyrights": [ ],
 "holders": [ ],
 "authors": [ ]
}
```
\end{lstlisting}


Die Ergebnisse zeigen, dass ein signifikanter Anteil der Modelle für fast alle Ausgaben auf den Parsing-Mechanismus angewiesen war.
Dies zeigt, dass ein Großteil der Antworten ohne Nachbearbeitung unbrauchbar gewesen wäre.
Auffällig ist, dass lediglich \texttt{qwen2.5-coder:32b} in der Lage war, ohne jegliches Parsing in allen Fällen korrekte JSON-Strukturen gemäß der Vorgaben des Eingabeprompts zu generieren.

Eine detaillierte Übersicht der parsing-bedingten Korrekturen pro Modell befindet sich in Tabelle~\ref{tab:extraction-beneficial} im Anhang.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Ungültige Ausgaben}

Trotz der Integration von Parsing-Mechanismen lieferten mehrere Modelle ungültige \gls{json}-Ausgaben.
Die Analyse dieser Fehler zeigt, dass die \glspl{llm} teils zusätzliche Felder generierten oder die erwartete Struktur fehlerhaft erweiterten.
Ein Beispiel hierfür ist \texttt{dolphin3:8b}, das Autoren nicht als String, sondern als Objekt mit den Attributen \textit{name} und \textit{mail\_address} darstellte:

\begin{lstlisting}[keepspaces=true]
{
"copyrights":[
    "Copyright 2012 Dialog Semiconductors Ltd.","Copyright 2013 Philipp Zabel, Pengutronix"
],
"holders":["Dialog Semiconductors Ltd.","Philipp Zabel","Pengutronix"],
"authors":[{"name":"Krystian Garbaciak","mail_address":"krystian.garbaciak@diasemi.com"}]
}
\end{lstlisting}

Dasselbe Fehlverhalten trat auch bei \texttt{olmo2:7b} auf, was maßgeblich zur hohen Anzahl ungültiger Ausgaben dieses Modells beitrug.
Besonders auffällig war zudem \texttt{orca-mini:3b}, das regelmäßig neue Felder wie \textit{maintainers} oder \textit{license} einfügte, obwohl diese im Eingabeprompt ausdrücklich als nicht relevant ausgeschlossen worden waren.
Ein Beispiel für eine fehlerhafte Ausgabe von \texttt{orca-mini:3b} ist im Folgenden dargestellt:

\begin{lstlisting}[keepspaces=true]
{
    "license": "GPL",
    "maintainers": [],
    "copyright": "Copyright (c) 2021, XYZ Company. All rights reserved.",
    "holders": [],
    "contributors": [],
    "authors": [
        {
        "mail": "michael.niedermayer@example.com",
        "name": "Michael Niedermayer"
}]}
\end{lstlisting}

Die Übersicht in Tabelle~\ref{tab:model-invalid-jsons} verdeutlicht, dass ausschließlich Modelle mit einer Parametergröße von bis zu 8 Milliarden Tokens fehlerhafte Strukturen erzeugten.
Dabei zeigt sich kein konsistenter Trend in Bezug auf den Modellprovider.
Auffällig ist jedoch, dass die drei Modelle mit den meisten fehlerhaften Ausgaben (\texttt{dolphin3:8b}, \texttt{olmo2:7b}, \texttt{orca-mini:3b}) aus vergleichsweise kleineren Projekten stammen und nicht von etablierten Providern entwickelt wurden.

Die Fähigkeit, das vorgegebene Ausgabeformat exakt zu reproduzieren, ist eine zentrale Voraussetzung für den Einsatz eines \glspl{llm} im Kontext der Copyright-Extraktion.
Modelle, die dabei Schwierigkeiten zeigen, sind daher für eine finale Implementierung nicht geeignet.

\begin{table}[H]
    \centering
    \begin{tabular}{l c}
        \toprule
        \textbf{Modell} & \textbf{Ungültige JSONs} \\
        \midrule
        deepseek-coder:6.7b     & 15 \\
        dolphin3:8b             & 45 \\
        olmo2:7b                & 24 \\
        orca-mini:3b            & 22 \\
        qwen2.5-coder:1.5b      & 9  \\
        \bottomrule
    \end{tabular}
    \caption{Anzahl ungültiger Ausgaben pro Modell}
    \label{tab:model-invalid-jsons}
\end{table}

% ----------------------------------------------------------------------------------------------------------------------

\subsection{Exact Matches}

Die \textit{Exact Matches} stellen im Benchmark eine zentrale Metrik dar, da sie unmittelbar Aufschluss über die Fähigkeit eines \glspl{llm} geben, Copyright-Statements entsprechend der \nameref{subsec:cep-01} zu extrahieren.
In Kombination mit der Geschwindigkeit der Token-Generierung (\textit{Tokens/sec}) wird sichtbar, welches Modell nicht nur präzise, sondern auch effizient arbeitet.
Abbildung~\ref{fig:exact-matches-overall-result} zeigt den Prozentsatz der \textit{Exact Matches} in Abhängigkeit von der Generierungsgeschwindigkeit für jedes Modell.

\begin{figure}[ht]
    \centering
    \makebox[\textwidth]{\includegraphics[width=1.3\textwidth]{benchmark/exact_matches_vs_tokens_per_sec.png}}
    \caption{Vergleich des F1-Score in Relation zu den Tokens/sec pro Modell. Die verschiedenen Varianten der Mistral-Familie erzielen durchweg die höchsten Werte im Benchmark.}
    \label{fig:exact-matches-overall-result}
\end{figure}

Die Ergebnisse verdeutlichen, dass kleinere Modelle tendenziell deutlich geringere Werte bei den \textit{Exact Matches} erreichen.
So liegt \texttt{tinyllama:1.1b} mit lediglich \num{3.43}\% am unteren Ende.
Im Kontrast dazu erzielte \texttt{mistral-small:24b} mit \num{94}\% den höchsten Wert, gefolgt von weiteren eng verwandten Varianten wie \texttt{devstral:24b} und \texttt{mistral-small3.2:24b}.
Dies bestätigt, dass größere Modelle eine deutlich höhere Zuverlässigkeit in der exakten Reproduktion erreichen.

Die Ergebnisse bilden klare Cluster entlang der Parametergrößen:
\begin{itemize}
    \item Kleine Modelle (bis etwa 3 Milliarden Parameter) generierten in vielen Fällen sehr schnell Tokens, erzielten aber nur sehr geringe Anteile an \textit{Exact Matches}. Beispiele hierfür sind \texttt{qwen2.5-coder:0.5b} mit über \num{100} Tokens/sec, aber nur \num{6.42}\% \textit{Exact Matches}, oder \texttt{tinyllama:1.1b} mit ähnlich schwacher Leistung.
    \item Mittelgroße Modelle im Bereich von 7 bis 12 Milliarden Parametern zeigten bemerkenswerterweise alle eine sehr ähnliche Geschwindigkeit von etwa \num{20} Tokens/sec. Unterschiede zeigten sich hier hauptsächlich in der Genauigkeit, die zwischen \num{20}\% (\texttt{dolphin3:8b}) und über \num{80}\% (\texttt{mistral:7b}) streute.
    \item Große Modelle ab 24B Parametern dominierten das obere Ende der Skala und erzielten durchweg hohe Genauigkeiten (über \num{85}\%), allerdings bei deutlich reduzierter Geschwindigkeit. Lediglich \texttt{qwen2.5-coder:32b} und \texttt{mixtral8x7b} weichen davon ab und erzielten mittelmäßige bis unbrauchbare Ergebnisse.
\end{itemize}

Besonders auffällig sind die Reasoning-Modelle \texttt{qwen3:4b} und \texttt{qwen3:8b}, die trotz ihrer geringen Parametergröße extrem langsam waren (\num{0.24} bzw. \num{0.19} Tokens/sec) und gleichzeitig unzureichende Genauigkeitswerte erreichten (\num{13.58}\% bzw. \num{7.69}\%). Dies deckt sich mit den besonders häufigen Timeouts dieser Modelle.
Ein gegenteiliger Fall ist \texttt{llama4:16x17b}, das zwar zu den langsamsten Modellen zählt, aber dennoch \num{88}\% \textit{Exact Matches} erreichte.
Im Gegensatz zu den Reasoning-Modellen ist \texttt{llama4:16x17b} mit 109 Milliarden Parametern das mit Abstand größte Modell des Benchmarks, erzielte aber dennoch geringere Leistungswerte als alle \texttt{mistral}-Varianten mit nur 24 Milliarden Parametern.

Innerhalb der Mistral-Familie zeigt sich eine deutliche Entwicklung: während \texttt{mistral:7b} im ersten Durchlauf mit \num{80.69}\% noch das beste Ergebnis erzielte, steigerten die Varianten mit 24 Milliarden Parametern die Genauigkeit bis auf \num{94}\%.
Bei den Varianten von \texttt{qwen2.5-coder} zeigt sich, dass die kleineren Modelle (0.5B, 1.5B, 3B) erwartungsgemäß zwar deutlich schneller, jedoch weniger präzise arbeiteten als ihre größeren Pendants.
Auffällig ist jedoch, dass die \texttt{qwen2.5-coder:7b}-Variante mit \num{74,68}\% den höchsten Wert an \textit{Exact Matches} erreichte.
Entgegen der Erwartungen erzielte die 14b-Variante lediglich \num{69,53}\%, während die 32b-Variante sogar auf \num{50,21}\% zurückfiel.
Dies verdeutlicht, dass eine höhere Parameterzahl nicht zwangsläufig zu besseren Ergebnissen innerhalb derselben Modellfamilie führt.

Die große Spannweite der Ergebnisse von nur \num{3}\% bis zu \num{94}\% \textit{Exact Matches} liefert einen wichtigen Hinweis auf die grundsätzlich Lösbarkeit des Benchmarks.
Wäre die Aufgabe unlösbar oder fehlerhaft gestellt gewesen, hätten alle Modelle an einer oberen Grenze stagniert.
Dies bestätigt die Validität des Benchmarks und seinen Nutzen für die Modellbewertung.

Zusammenfassend lässt sich festhalten, dass insbesondere größere Modelle wie die Mistral-Varianten oder \texttt{llama4:16x17b} die besten Ergebnisse liefern, da sie eine hohe Genauigkeit erreichen, auch wenn sie dafür Geschwindigkeit einbüßen.
Kleine Modelle punkten zwar mit hoher Geschwindigkeit, verfehlen aber die geforderte Präzision.
Die Reasoning-orientierten Qwen3-Modelle sind aufgrund ihrer extrem niedrigen Geschwindigkeit und unzureichenden Genauigkeit klar auszuschließen.

% ----------------------------------------------------------------------------------------------------------------------

\subsection{F1-Score}

Im Gegensatz zu den \textit{Exact Matches}, die ausschließlich eine exakte Reproduktion der erwarteten Copyright-Statements bewerten, berücksichtigt der F1-Score zusätzlich die Korrektheit bei der Extraktion von \textit{Holders} und \textit{Authors}.
Damit bietet der F1-Score eine andere Perspektive bei der Bewertung der Modellqualität, da er auch semantisch korrekte, aber leicht abweichende Formulierungen anerkennt.

Die Abbildung~\ref{fig:f1-overall-result} zeigt den Zusammenhang zwischen dem F1-Score und der Geschwindigkeit in \textit{Tokens/sec} für alle Modelle.
Im Vergleich zu den Ergebnissen der \textit{Exact Matches} wird deutlich, dass viele Modelle durch den F1-Score ein robusteres Bild abgeben.
Während die Spannweite bei den \textit{Exact Matches} von nur rund \num{3}\% bis \num{94}\% reichte, bewegen sich die meisten Modelle im F1-Score zwischen \num{0.6} und \num{0.9}.
Dies deutet darauf hin, dass auch Modelle mit schwacher Performance bei den \textit{Exact Matches} dennoch relevante Teilergebnisse liefern konnten, die in den F1-Score einfließen.

\begin{figure}[ht]
    \centering
    \makebox[\textwidth]{\includegraphics[width=1.3\textwidth]{benchmark/f1_vs_tokens_per_sec.png}}
    \caption{Vergleich des \textit{F1-Score} in Relation zu den \textit{Tokens/sec} pro Modell. Die Berücksichtigung der \textit{Holders} und \textit{Authors} hat bei vielen Modellen eine Verbesserung des Ergebnisse im Vergleich zu den \textit{Exact Matches} zur Folge.}
    \label{fig:f1-overall-result}
\end{figure}

Auffällig ist, dass sich die Spitzenmodelle weitgehend decken. \texttt{Mistral-small:24b}, \texttt{mistral-small3.2:24b}, \texttt{devstral:24b} und \texttt{llama4:16x17b} erzielen mit F1-Scores um die \num{0.9} die besten Resultate, die ihren hervorragenden Werten bei den \textit{Exact Matches} entsprechen.
Damit bestätigen sie ihre Eignung sowohl für die exakte als auch für die semantisch korrekte Extraktion.

Die Reasoning-Modelle \texttt{qwen3:4b} und \texttt{qwen3:8b}, die bei den \textit{Exact Matches} am unteren Ende lagen, zeigen auch hier schwache Resultate (\num{0.49} bzw. \num{0.34}).

Eine weitere interessante Beobachtung betrifft die kleineren Varianten von \texttt{qwen2.5-coder}.
Während die Modelle mit 0.5B und 1.5B Parametern zwar sehr schnell arbeiteten (über 70 Tokens/sec), blieben ihre F1-Scores mit rund \num{0.42} bis \num{0.50} deutlich hinter den größeren Varianten zurück.
Die besten Resultate innerhalb dieser Modellfamilie erzielte erneut die 7B-Variante (\num{0.82}), während die 14B- und 32B-Varianten etwas abfielen.
Dies verdeutlicht erneut, dass ein größerer Parameterumfang nicht zwangsläufig zu einer besseren Leistung in einem spezifischen Anwendungsfall innerhalb einer Modellreihe führt.

Die 7B- und 8B-Modelle der meisten Anbieter (z.B.\ \texttt{mistral:7b}, \texttt{mathstral:7b}, \texttt{llava:7b}, \texttt{gemma3:4b}, \texttt{llama3:8b}) zeigen einen F1-Score im Bereich von \num{0.6} bis \num{0.8} bei relativ stabilen Geschwindigkeiten um die 20 Tokens/sec.
Dies bestätigt den zuvor beobachteten Cluster und verdeutlicht, dass mittlere Modellgrößen eine solide, wenn auch nicht führende Performance liefern.

Insgesamt zeigt sich, dass die Berücksichtigung der \textit{Authors} und \textit{Holders} sowie die Akzeptanz von nahezu idealen Ergebnissen (Similarity-Threshold) engere Verteilung der Ergebnisse verursacht.
Zwar scheitern einige Modelle bei einer exakten Extraktion anhand der Aufgabenbeschreibung aber dennoch sind sie imstande, leicht abweichende Ergebnisse zu erzielen.

Insgesamt zeigt sich, dass die Einbeziehung der \textit{Authors} und \textit{Holders} sowie die Berücksichtigung nahezu identischer Ergebnisse durch den Similarity-Threshold zu einer konsistenteren und weniger stark streuenden Ergebnisverteilung führt.
Zwar verfehlen einige Modelle die exakte Extraktion gemäß der Aufgabenbeschreibung, sind jedoch dennoch in der Lage, leicht abweichende Ergebnisse zu liefern.

% ======================================================================================================================

\section{Wahl des Modells}\label{sec:auswahl-modell-benchmark}

Auf Grundlage der in Abschnitt~\ref{sec:ergebnisse-benchmark} dargestellten Ergebnisse fiel die finale Wahl auf das Modell \texttt{mistral-small:24b}.
Ausschlaggebend hierfür waren mehrere Faktoren:

Erstens zeigte \texttt{mistral-small:24b} eine außerordentliche Robustheit in der Durchführung des Benchmarks.
Im Gegensatz zu zahlreichen anderen Modellen kam es weder zu Timeouts noch zu ungültigen \gls{json}-Ausgaben.

Zweitens erzielte \texttt{mistral-small:24b} mit einem Anteil von \num{94}\% \textit{Exact Matches} den höchsten Wert aller getesteten Modelle.
Auch im F1-Score erreichte es Spitzenwerte nahe \num{0.9}, was seine Fähigkeit unterstreicht, nicht nur exakte Übereinstimmungen, sondern auch semantisch konsistente Extraktionen von \textit{Authors} und \textit{Holders} vorzunehmen.

Ein drittes entscheidendes Kriterium war die Lizenzierung.
\texttt{Mistral-small:24b} wird unter der Apache 2.0 Lizenz bereitgestellt, welche eine uneingeschränkte kommerzielle Nutzung, Modifikation und Integration erlaubt.
Diese Lizenz passt optimal zu den Anforderungen der geplanten Anwendung und stellt sicher, dass keine rechtlichen Einschränkungen den Einsatz des Modells behindern.

Darüber hinaus spielte auch der Ursprung des Modells eine Rolle, da die Wahl eines europäischen Unternehmens wie Mistral durch Aspekte der digitalen Souveränität gestützt wird.
Ein europäisches Modell stärkt nicht nur die Unabhängigkeit von außereuropäischen Anbietern, sondern erleichtert zugleich die Einhaltung europäischer Datenschutz- und Compliance-Vorgaben.

Zusammenfassend vereint \texttt{mistral-small:24b} eine hohe Extraktionsqualität, robuste Ergebnisse ohne strukturelle Fehler, eine vorteilhafte Lizenzierung und die Unterstützung durch einen europäischen Anbieter.
Diese Faktoren machten es zum klar geeigneten Modell für die finale Implementierung.
