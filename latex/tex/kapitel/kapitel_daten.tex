\chapter{Daten}\label{ch:daten}

In diesem Kapitel wird die Aggregation von Test- und Evaluierungsdaten erläutert.
Als Datenquelle dient das Scan"=Ergebnis eines Alpine Containers.
Das Scan"=Ergebnis beinhaltet sämtliche im Container enthaltenen Quellcode"=Dateien zusammen mit dem entsprechenden MetaScan"=Ergebnissen sowie die durch den ScanCode-Service erzeugten Metadaten.

% Hier wird beschrieben, welche Datenquelle ausgewählt wurde, warum sie ausgewählt wurde und welche Alternativen es gab.
\section{Wahl der Datenquelle}\label{sec:wahl-der-datenquelle}

% TODO: Quelle einfügen
Das Github Repository des ScanCode Toolkit beinhaltet einen Testdatensatz zur Erkennung von Copyrights.
Dieser Datensatz besteht aus mehreren tausend Dateien und ihren zugehörigen ScanCode"=Ergebnissen.
Die Daten sind zum Teil in besondere Einzelfälle untergliedert, der Großteil der Daten ist jedoch unsortiert.
Da der Datensatz zum Testen der ScanCode Copyright Implementierung dient, beinhaltet er sowohl künstlich erzeugte Testfälle, als auch solche, die von Realdaten abgeleitet sind.

In Absprache mit dem Stakeholder wurde die Absicht, den Testdatensatz des ScanCode Toolkit zu verwenden verworfen und stattdessen eine bessere Datenquelle herangezogen.
Als Alternative Datenquelle wurde ein Alpine Container gewählt.
Dieser Container wurde mit Hilfe des Metaeffekt Scanners analysiert und inventarisiert.
Der eindeutige Vorteil dieser Datenquelle besteht darin, dass es sich hierbei ausschließlich um Realdaten handelt, wie sie im Produktivbetrieb vorkommen.
Das Scan-Ergebnis ist außerdem um ein Vielfaches größer als der Testdatensatz des ScanCode Toolkit.

\section{Erzeugung des Ausgangsdatensatzes}\label{sec:erzeugung-datensatz}

Beim Scan des Alpine Containers mit Hilfe des Metaeffekt Scanners laufen mehrere Schritte ab, die für die Aggregation eines Copyright"=Datensatzes relevant sind.
Zunächst werden sämtliche Datenpakete entpackt und die resultierenden Quellcode"=Dateien in eines oder mehrere Segmente unterteilt.
Die resultierenden Segmente werden normalisiert und auf diese normalisierten Segmente werden Scan"=Prozesse, wie der ScanCode"=Service, angewandt.
Das durch den ScanCode"=Service erzeugte Ergebnis beinhaltet somit die gefundenen Copyright"=Informationen für ein normalisiertes Segment.
Die Normalisierung der Segmente beinhaltet unter anderem das Entfernen von mehrfachen Leerzeichen sowie Zeilenumbrüchen.
Die Copyright Policy besagt, dass Copyright Statements exakt so extrahiert werden sollen, wie sie in der Originaldatei vorkommen.
Da die Normalisierung und Segmentierung den originalen Zustand der Datei verändern, können sie nicht für den Datensatz herangezogen werden.
Stattdessen werden die in den Segmenten referenzierten Originaldateien ermittelt und für jede Datei die Scan"=Ergebnisse aller ihrer Segmente zusammengeführt.

% TODO: Eventuell hier kurz darauf eingehen, dass der SHA-1 Hash gewählt wurde, um kollisionen zu vermeiden und von originalen Dateinamen zu abstrahieren. Wahrscheinlichkeit für Kollision nennen.
Um Namenskonflikte zu vermeiden, wurde anschließend für jede Originaldatei der SHA-1 Hash berechnet und zusammen mit dem Dateityp zur Benennung der Datei verwendet.
Da die Verzeichnisstruktur der Originaldateien nicht relevant ist, werden alle Dateien in einem flachen Verzeichnis gespeichert, dies vereinfacht die anschließende Verarbeitung zusätzlich.
Der resultierende Ausgangsdatensatz umfasst etwa 467000 Originaldateien und ihre Scan"=Ergebnisse.

\section{Analyse des Datensatzes und Kategorisierung der Daten}\label{sec:analyse-datensatz}

Der erzeugte Datensatz kann in seiner extrahierten Form noch nicht zur Evaluation und Testierung verwendet werden, da die Scan"=Ergebnisse nicht auf den Originaldateien, sondern auf ihren normalisierten Segmenten beruhen.
Um diese Problematik zu adressieren, muss ein Prozess bestimmt werden, der bereits korrekte Daten ermittelt und fehlerhafte Daten, sofern möglich, korrigiert.
Dieser Prozess wird nachfolgend schrittweise beschrieben:

Im ersten Schritt der Datenaggregation wurde der Datensatz in zwei Kategorien unterteilt: Dateien, für die ScanCode \enquote{copyrights}, \enquote{holders} bzw. \enquote{authors} ermitteln konnte, und solche, die keine der genannten Informationen enthalten.
Etwa zwei drittel des Datensatzes gehören zur letzteren Kategorie.
Die Daten ohne ScanCode"=Ergebnis sind allerdings dennoch relevant, da es sich hierbei um False-Negatives handeln kann, die zur Verbesserung der Extraktion unbedingt ermittelt werden müssen.
Im zweiten Schritt wurden zunächst Duplikate aus dem Datensatz mit ScanCode"=Ergebnissen entfernt.
Hierzu wurden Daten mit demselben SHA-1 Hash ermittelt und alle Duplikate entfernt, diese Filterung reduzierte den Datensatz um etwa 8\%.
% TODO: Überlegen ob "Bias" hier richtig ist und wie man es evtl. besser erklären kann
Das Entfernen von Duplikaten reduziert nicht nur die Redundanz, sondern verhindert auch, dass Modelle, die auf diesen Daten trainiert oder evaluiert werden, einen Bias entwickeln.
Der von Duplikaten bereinigte Datensatz wurde im nächsten Schritt danach unterteilt, ob das ScanCode Ergebnis ausschließlich \enquote{authors} enthält.
Diese Kategorie umfasst etwa 11000 Dateien und dient dazu, die Extraktion auf False-Positives beim Erkennen der \enquote{copyrights} zu analysieren.

% Hier soll darauf eingegangen werden, welche Probleme und Herausforderungen der Datensatz mit sich gebracht hat, einige
% davon sind die Größe (170GB), die Encoding, Binary-Dateien und die geringe Qualität der Scancode extraktion im "authors" Feld.
\section{Herausforderungen bei der Datenaggregation}\label{sec:herausforderungen-datenaggregation}

% Hier soll darauf eingegangen werden, welche Probleme/Qualitätseinbußen der Datensatz aufzuweisen hat, darüberhinaus soll erläutert werden, warum der Datensatz gut ist
% Der Datensatz ist nicht gut weil es keinen besseren gibt -> vielleicht ist ein besserer nur nicht bekannt, stattdessen
% ist der Datensatz mit dem aktuellen Industriestandard (ScanCode) erzeugt worden und wurde nach der von uns erstellten Policy verbessert und validiert
\section{Qualität der Daten}\label{sec:qualitat-der-daten}


