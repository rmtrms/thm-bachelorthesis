
@article{breton_empowering_2024,
	title = {Empowering {CamemBERT} Legal Entity Extraction With {LLM} Boostrapping},
	doi = {10.1007/978-3-031-77792-9_6},
	abstract = {The legal industry is characterized by the presence of large volumes and complex documents. Given the continuous evolution of these documents, there is a growing interest in automating the processing of legal texts to streamline compliance. One key step of this process is the extraction of legal entities. State-of-the-art methods for legal entity extraction, including rule-based systems, Bi-{LSTM}, and {BERT}, require substantial annotated data to be effective, a task that is time-intensive for domain experts. With the rise of Large Language Models ({LLMs}), research has increasingly focused on leveraging their capabilities and exploring zero-shot approaches. In this paper, we present a hybrid system that distils {GPT}-4 knowledge through rule-based methods into a {CamemBERT} model. This approach not only reduces the need for expert involvement compared to the standard {CamemBERT} system but also outperforms the {GPT}-4-only system, enhancing the F1 score for legal entities by 9-24\% points.},
	pages = {86--101},
	journaltitle = {{EKAW} 2024},
	author = {Breton, Julien and Billami, Mokhtar Boumedyen and Chevalier, Max and Trojahn, Cassia},
	date = {2024-11-20},
	note = {Publisher: {CCSD}},
	keywords = {Large Language Models ({LLMs}), [{INFO}]Computer Science [cs], Amsterdam, Netherlands, Camem-{BERT}, Knowledge Distilation, Legal Entity Extraction, Limited Annotated Data},
	file = {Breton et al. - 2024 - Empowering CamemBERT Legal Entity Extraction With .pdf:/Users/romeo/Zotero/storage/NBTVDFRP/Breton et al. - 2024 - Empowering CamemBERT Legal Entity Extraction With .pdf:application/pdf},
}

@misc{pakhale_comprehensive_2023,
	title = {Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges},
	url = {http://arxiv.org/abs/2309.14084},
	doi = {10.48550/arXiv.2309.14084},
	shorttitle = {Comprehensive Overview of Named Entity Recognition},
	abstract = {In the domain of Natural Language Processing ({NLP}), Named Entity Recognition ({NER}) stands out as a pivotal mechanism for extracting structured insights from unstructured text. This manuscript offers an exhaustive exploration into the evolving landscape of {NER} methodologies, blending foundational principles with contemporary {AI} advancements. Beginning with the rudimentary concepts of {NER}, the study spans a spectrum of techniques from traditional rule-based strategies to the contemporary marvels of transformer architectures, particularly highlighting integrations such as {BERT} with {LSTM} and {CNN}. The narrative accentuates domain-specific {NER} models, tailored for intricate areas like finance, legal, and healthcare, emphasizing their specialized adaptability. Additionally, the research delves into cutting-edge paradigms including reinforcement learning, innovative constructs like E-{NER}, and the interplay of Optical Character Recognition ({OCR}) in augmenting {NER} capabilities. Grounding its insights in practical realms, the paper sheds light on the indispensable role of {NER} in sectors like finance and biomedicine, addressing the unique challenges they present. The conclusion outlines open challenges and avenues, marking this work as a comprehensive guide for those delving into {NER} research and applications.},
	number = {{arXiv}:2309.14084},
	publisher = {{arXiv}},
	author = {Pakhale, Kalyani},
	urldate = {2025-06-04},
	date = {2023-09-25},
	eprinttype = {arxiv},
	eprint = {2309.14084 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/4GPFMSPY/Pakhale - 2023 - Comprehensive Overview of Named Entity Recognition.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/TERZWKDY/2309.html:text/html},
}

@misc{dunn_structured_2022,
	title = {Structured information extraction from complex scientific text with fine-tuned large language models},
	url = {http://arxiv.org/abs/2212.05238},
	doi = {10.48550/arXiv.2212.05238},
	abstract = {Intelligently extracting and linking complex scientific information from unstructured text is a challenging endeavor particularly for those inexperienced with natural language processing. Here, we present a simple sequence-to-sequence approach to joint named entity recognition and relation extraction for complex hierarchical information in scientific text. The approach leverages a pre-trained large language model ({LLM}), {GPT}-3, that is fine-tuned on approximately 500 pairs of prompts (inputs) and completions (outputs). Information is extracted either from single sentences or across sentences in abstracts/passages, and the output can be returned as simple English sentences or a more structured format, such as a list of {JSON} objects. We demonstrate that {LLMs} trained in this way are capable of accurately extracting useful records of complex scientific knowledge for three representative tasks in materials chemistry: linking dopants with their host materials, cataloging metal-organic frameworks, and general chemistry/phase/morphology/application information extraction. This approach represents a simple, accessible, and highly-flexible route to obtaining large databases of structured knowledge extracted from unstructured text. An online demo is available at http://www.matscholar.com/info-extraction.},
	number = {{arXiv}:2212.05238},
	publisher = {{arXiv}},
	author = {Dunn, Alexander and Dagdelen, John and Walker, Nicholas and Lee, Sanghoon and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin and Jain, Anubhav},
	urldate = {2025-06-05},
	date = {2022-12-10},
	eprinttype = {arxiv},
	eprint = {2212.05238 [cs]},
	keywords = {Computer Science - Computation and Language, Condensed Matter - Materials Science},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/KUCKDM2F/Dunn et al. - 2022 - Structured information extraction from complex sci.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/CXQBSCLJ/2212.html:text/html},
}

@misc{wei_chatie_2024,
	title = {{ChatIE}: Zero-Shot Information Extraction via Chatting with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.10205},
	doi = {10.48550/arXiv.2302.10205},
	shorttitle = {{ChatIE}},
	abstract = {Zero-shot information extraction ({IE}) aims to build {IE} systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot {IE} reduces the time and effort that data labeling takes. Recent efforts on large language models ({LLMs}, e.g., {GPT}-3, {ChatGPT}) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong {IE} models can be constructed by directly prompting {LLMs}. Specifically, we transform the zero-shot {IE} task into a multi-turn question-answering problem with a two-stage framework ({ChatIE}). With the power of {ChatGPT}, we extensively evaluate our framework on three {IE} tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that {ChatIE} achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., {NYT}11-{HRL}). We believe that our work could shed light on building {IE} models with limited resources.},
	number = {{arXiv}:2302.10205},
	publisher = {{arXiv}},
	author = {Wei, Xiang and Cui, Xingyu and Cheng, Ning and Wang, Xiaobin and Zhang, Xin and Huang, Shen and Xie, Pengjun and Xu, Jinan and Chen, Yufeng and Zhang, Meishan and Jiang, Yong and Han, Wenjuan},
	urldate = {2025-06-05},
	date = {2024-05-27},
	eprinttype = {arxiv},
	eprint = {2302.10205 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/5YXWZLP5/Wei et al. - 2024 - ChatIE Zero-Shot Information Extraction via Chatt.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/EHGH3CR3/2302.html:text/html},
}

@misc{islam_llm-based_2025,
	title = {{LLM}-based Prompt Ensemble for Reliable Medical Entity Recognition from {EHRs}},
	url = {http://arxiv.org/abs/2505.08704},
	doi = {10.48550/arXiv.2505.08704},
	abstract = {Electronic Health Records ({EHRs}) are digital records of patient information, often containing unstructured clinical text. Named Entity Recognition ({NER}) is essential in {EHRs} for extracting key medical entities like problems, tests, and treatments to support downstream clinical applications. This paper explores prompt-based medical entity recognition using large language models ({LLMs}), specifically {GPT}-4o and {DeepSeek}-R1, guided by various prompt engineering techniques, including zero-shot, few-shot, and an ensemble approach. Among all strategies, {GPT}-4o with prompt ensemble achieved the highest classification performance with an F1-score of 0.95 and recall of 0.98, outperforming {DeepSeek}-R1 on the task. The ensemble method improved reliability by aggregating outputs through embedding-based similarity and majority voting.},
	number = {{arXiv}:2505.08704},
	publisher = {{arXiv}},
	author = {Islam, K. M. Sajjadul and Nipu, Ayesha Siddika and Wu, Jiawei and Madiraju, Praveen},
	urldate = {2025-06-14},
	date = {2025-05-25},
	eprinttype = {arxiv},
	eprint = {2505.08704 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/ENSALNXQ/Islam et al. - 2025 - LLM-based Prompt Ensemble for Reliable Medical Ent.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/4HM5SYIL/2505.html:text/html},
}

@misc{villena_llmner_2024,
	title = {{llmNER}: (Zero{\textbar}Few)-Shot Named Entity Recognition, Exploiting the Power of Large Language Models},
	url = {http://arxiv.org/abs/2406.04528},
	doi = {10.48550/arXiv.2406.04528},
	shorttitle = {{llmNER}},
	abstract = {Large language models ({LLMs}) allow us to generate high-quality human-like text. One interesting task in natural language processing ({NLP}) is named entity recognition ({NER}), which seeks to detect mentions of relevant information in documents. This paper presents {llmNER}, a Python library for implementing zero-shot and few-shot {NER} with {LLMs}; by providing an easy-to-use interface, {llmNER} can compose prompts, query the model, and parse the completion returned by the {LLM}. Also, the library enables the user to perform prompt engineering efficiently by providing a simple interface to test multiple variables. We validated our software on two {NER} tasks to show the library's flexibility. {llmNER} aims to push the boundaries of in-context learning research by removing the barrier of the prompting and parsing steps.},
	number = {{arXiv}:2406.04528},
	publisher = {{arXiv}},
	author = {Villena, Fabián and Miranda, Luis and Aracena, Claudio},
	urldate = {2025-06-14},
	date = {2024-06-06},
	eprinttype = {arxiv},
	eprint = {2406.04528 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/VM28DBAB/Villena et al. - 2024 - llmNER (ZeroFew)-Shot Named Entity Recognition, .pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/EJYPISAY/2406.html:text/html},
}

@misc{hu_improving_2024,
	title = {Improving Large Language Models for Clinical Named Entity Recognition via Prompt Engineering},
	url = {http://arxiv.org/abs/2303.16416},
	doi = {10.48550/arXiv.2303.16416},
	abstract = {Objective: This study quantifies the capabilities of {GPT}-3.5 and {GPT}-4 for clinical named entity recognition ({NER}) tasks and proposes task-specific prompts to improve their performance. Materials and Methods: We evaluated these models on two clinical {NER} tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the {MTSamples} corpus, following the 2010 i2b2 concept extraction shared task, and (2) identifying nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system ({VAERS}). To improve the {GPT} models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to {BioClinicalBERT}. Results: Using baseline prompts, {GPT}-3.5 and {GPT}-4 achieved relaxed F1 scores of 0.634, 0.804 for {MTSamples}, and 0.301, 0.593 for {VAERS}. Additional prompt components consistently improved model performance. When all four components were used, {GPT}-3.5 and {GPT}-4 achieved relaxed F1 socres of 0.794, 0.861 for {MTSamples} and 0.676, 0.736 for {VAERS}, demonstrating the effectiveness of our prompt framework. Although these results trail {BioClinicalBERT} (F1 of 0.901 for the {MTSamples} dataset and 0.802 for the {VAERS}), it is very promising considering few training samples are needed. Conclusion: While direct application of {GPT} models to clinical {NER} tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances {GPT} models' feasibility for potential clinical applications.},
	number = {{arXiv}:2303.16416},
	publisher = {{arXiv}},
	author = {Hu, Yan and Chen, Qingyu and Du, Jingcheng and Peng, Xueqing and Keloth, Vipina Kuttichi and Zuo, Xu and Zhou, Yujia and Li, Zehan and Jiang, Xiaoqian and Lu, Zhiyong and Roberts, Kirk and Xu, Hua},
	urldate = {2025-06-14},
	date = {2024-01-25},
	eprinttype = {arxiv},
	eprint = {2303.16416 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/ESY5GXM2/Hu et al. - 2024 - Improving Large Language Models for Clinical Named.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/CJBNURAR/2303.html:text/html},
}

@online{noauthor_chatutil-yan-wittmann_nodate,
	title = {{ChatUtil}-Yan-Wittmann},
	url = {https://github.com/YanWittmann/automatic-document-classification/blob/main/src/main/java/de/yanwittmann/document/ai/ChatUtil.java},
	abstract = {{OCR}/{AI}-powered automated document renaming and sorting - {YanWittmann}/automatic-document-classification},
	titleaddon = {{GitHub}},
	urldate = {2025-07-06},
	langid = {english},
}

@inreference{noauthor_f-score_2025,
	title = {F-score},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=F-score&oldid=1296408310},
	booktitle = {Wikipedia},
	urldate = {2025-07-06},
	date = {2025-06-19},
	langid = {english},
	note = {Page Version {ID}: 1296408310},
}

@online{noauthor_what_nodate,
	title = {What are tokens and how to count them? {\textbar} {OpenAI} Help Center},
	url = {https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them},
	shorttitle = {What are tokens and how to count them?},
	urldate = {2025-07-06},
	langid = {english},
}

@online{noauthor_scancode-toolkit-documentation_nodate,
	title = {{ScanCode}-Toolkit-Documentation},
	url = {https://scancode-toolkit.readthedocs.io/en/latest/getting-started/home.html},
	urldate = {2025-07-06},
	file = {Home — ScanCode-Toolkit documentation:/Users/romeo/Zotero/storage/TXFK69CE/home.html:text/html},
}

@online{noauthor_ollama_nodate,
	title = {Ollama},
	url = {https://ollama.com},
	abstract = {Get up and running with large language models.},
	urldate = {2025-07-06},
	file = {Snapshot:/Users/romeo/Zotero/storage/CEGHQQ3F/ollama.com.html:text/html},
}

@software{noauthor_metaeffekt-scancode-service_2025,
	title = {Metaeffekt-{ScanCode}-Service},
	rights = {Apache-2.0},
	url = {https://github.com/org-metaeffekt/metaeffekt-scancode-service},
	abstract = {Local service for efficient integration of {ScanCode} Toolkit.},
	publisher = {\{metæffekt\}},
	urldate = {2025-07-06},
	date = {2025-05-14},
	note = {original-date: 2024-03-20T12:57:22Z},
	keywords = {scancode},
}

@unpublished{gang_smarter_2025,
	title = {Smarter Fine-Tuning: How {LoRA} Enhances Large Language Models},
	url = {https://hal.science/hal-04983079},
	shorttitle = {Smarter Fine-Tuning},
	abstract = {The rapid advancement of Large Language Models ({LLMs}) has revolutionized natural language processing ({NLP}) and various {AI}-driven applications. However, the fine-tuning of such massive models remains computationally expensive, limiting their adaptability to domain-specific tasks. Low-Rank Adaptation ({LoRA}) has emerged as a prominent parameter-efficient fine-tuning ({PEFT}) technique that significantly reduces memory and computational overhead by introducing trainable low-rank matrices while freezing most of the pre-trained model parameters. {LoRA} enables efficient model adaptation without compromising performance, making it an attractive alternative to full fine-tuning. This survey provides a comprehensive overview of {LoRA}, including its theoretical foundations, integration into transformer-based architectures, and comparative advantages over traditional fine-tuning techniques. We explore its applications across diverse domains, including {NLP}, code generation, healthcare, finance, and multimodal {AI}. Additionally, we examine real-world case studies that demonstrate {LoRA}'s effectiveness in optimizing computational costs while preserving model performance. Despite its numerous benefits, {LoRA} presents several challenges, including optimal rank selection, generalization across multiple tasks, and its dependency on pre-trained model capabilities. We discuss these limitations and highlight promising future research directions, such as adaptive rank estimation, multimodal extensions, federated learning integration, and energy-efficient variants. By bridging the gap between efficiency and adaptability, {LoRA} represents a pivotal advancement in democratizing {LLM} fine-tuning. As {AI} models continue to scale, {LoRA} will play an essential role in enabling cost-effective and scalable adaptation, driving innovation in {AI} applications across industries.},
	author = {Gang, Yue and Shun, Jianhong and Qing, Mu},
	urldate = {2025-07-20},
	date = {2025-03},
	keywords = {Deep Learning, Large Language Models, {AI} Optimization, Computational Efficiency, Low-Rank Adaptation, Model Adaptation, {NLP}, Parameter-Efficient Fine-Tuning, Transfer Learning, Transformer Models},
	file = {HAL PDF Full Text:/Users/romeo/Zotero/storage/C3E4NUVX/Gang et al. - 2025 - Smarter Fine-Tuning How LoRA Enhances Large Langu.pdf:application/pdf},
}

@article{benlahbib_comparative_2025,
	title = {Comparative Analysis of Traditional and Modern {NLP} Techniques on the {CoLA} Dataset: From {POS} Tagging to Large Language Models},
	volume = {6},
	issn = {2644-1268},
	url = {https://ieeexplore.ieee.org/document/10829978},
	doi = {10.1109/OJCS.2025.3526712},
	shorttitle = {Comparative Analysis of Traditional and Modern {NLP} Techniques on the {CoLA} Dataset},
	abstract = {The task of classifying linguistic acceptability, exemplified by the {CoLA} (Corpus of Linguistic Acceptability) dataset, poses unique challenges for natural language processing ({NLP}) models. These challenges include distinguishing between subtle grammatical errors, understanding complex syntactic structures, and detecting semantic inconsistencies, all of which make the task difficult even for human annotators. In this article, we compare a range of techniques, from traditional methods such as Part-of-Speech ({POS}) tagging and feature extraction methods like {CountVectorizer} with Term Frequency-Inverse Document Frequency ({TF}-{IDF}) and N-grams, to modern embeddings such as {FastText} and Embeddings from Language Models ({ELMo}), as well as deep learning architectures like transformers and Large Language Models ({LLMs}). Our experiments show a clear improvement in performance as models evolve from traditional to more advanced approaches. Notably, state-of-the-art ({SOTA}) results were obtained by fine-tuning {GPT}-4o with extensive hyperparameter tuning, including experimenting with various epochs and batch sizes. This comparative analysis provides valuable insights into the relative strengths of each technique for identifying morphological, syntactic, and semantic violations, highlighting the effectiveness of {LLMs} in these tasks.},
	pages = {248--260},
	journaltitle = {{IEEE} Open Journal of the Computer Society},
	author = {Benlahbib, Abdessamad and Boumhidi, Achraf and Fahfouh, Anass and Alami, Hamza},
	urldate = {2025-07-20},
	date = {2025},
	keywords = {Data models, natural language processing, Benchmark testing, Computational modeling, Correlation coefficient, Large language models, Large language models ({LLMs}), linguistic acceptability, Linguistics, Natural language processing, Syntactics, Tagging, Transformers},
	file = {Full Text PDF:/Users/romeo/Zotero/storage/JZW9VQJZ/Benlahbib et al. - 2025 - Comparative Analysis of Traditional and Modern NLP.pdf:application/pdf},
}

@article{siino_exploring_2025,
	title = {Exploring {LLMs} Applications in Law: A Literature Review on Current Legal {NLP} Approaches},
	volume = {13},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10850911},
	doi = {10.1109/ACCESS.2025.3533217},
	shorttitle = {Exploring {LLMs} Applications in Law},
	abstract = {Artificial Intelligence ({AI}) is reshaping the legal landscape, with software tools now impacting various aspects of legal work. The intersection of Natural Language Processing ({NLP}) and law holds potential to transform how legal professionals, including lawyers and judges, operate, resolve disputes, and retrieve case information to formulate their decisions. To identify the current state of the applications of Transformers (also known as Large Language Models or {LLMs}) in the legal domain, we analysed the existing literature from 2017 to 2023 through a database search and snowballing method. From 61 selected publications, we identified key application categories such as legal document analysis, case prediction, and contract review, along with their main characteristics. We observed a discernible upsurge in the volume of scholarly publications, a diversification of tasks undertaken (e.g., legal research, contract analysis, and regulatory compliance), and an increased range of languages considered. There has been a notable enhancement in the methodological sophistication employed by researchers in practical applications. The performance of models grounded in the Generative Pre-trained Transformer ({GPT}) architecture has consistently improved across various legal domains, including contract review, legal document summarization, and case outcome prediction. This paper makes several significant contributions to the field. Firstly, it identifies emerging trends in the application of {LLMs} within the legal domain, highlighting the growing interest and investment in this area. Secondly, it pinpoints methodological gaps in current research, suggesting areas where further development and refinement are needed. Lastly, it discusses the broader implications of these advancements for real-world legal tasks, offering insights into how {LLM}-based {AI} can enhance legal practice while addressing the associated challenges.},
	pages = {18253--18276},
	journaltitle = {{IEEE} Access},
	author = {Siino, Marco and Falco, Mariana and Croce, Daniele and Rosso, Paolo},
	urldate = {2025-07-20},
	date = {2025},
	keywords = {{GPT}, Artificial intelligence, Natural language processing, Transformers, {AI} for law, Attention mechanisms, Contracts, Databases, law, Law, legal {NLP}, legal tech, literature review, Quality assessment, Question answering (information retrieval), Reliability, Systematic literature review, transformers},
	file = {Full Text PDF:/Users/romeo/Zotero/storage/J8IGWHI2/Siino et al. - 2025 - Exploring LLMs Applications in Law A Literature R.pdf:application/pdf},
}

@article{noauthor_open-source-leitfaden_nodate,
	title = {Open-Source-Leitfaden},
	langid = {german},
	file = {Bitkom, Open Source Leitfaden, Version 3.2:/Users/romeo/Zotero/storage/5I9AP2C8/Open-Source-Leitfaden.pdf:application/pdf},
}

@online{meckel_definition_nodate,
	title = {Definition: Welt-Urheberrechts-Abkommen},
	url = {https://wirtschaftslexikon.gabler.de/definition/welt-urheberrechts-abkommen-50140},
	shorttitle = {Definition},
	abstract = {Universal Copyright Convention. Am 6.9.1952 unterzeichnetes, am 24.7.1971 revidiertes, aufgrund Vertragsgesetz vom 17.8.1973 ({BGBl}. {II} 1069) in der Bundesrepublik Deutschland am 10.7.1974 in Kraft getretenes ({BGBl}. {II} 1309) Abkommen, das den Werken fremder Autoren den gleichen Schutz gewähren soll},
	titleaddon = {https://wirtschaftslexikon.gabler.de/definition/welt-urheberrechts-abkommen-50140},
	type = {Text},
	author = {Meckel, Dr Astrid},
	urldate = {2025-07-24},
	langid = {german},
	note = {Publisher: Springer Fachmedien Wiesbaden {GmbH}
Section: economy},
}

@online{meckel_definition_nodate-1,
	title = {Definition: Revidierte Berner Übereinkunft ({RBÜ})},
	url = {https://wirtschaftslexikon.gabler.de/definition/revidierte-berner-uebereinkunft-rbue-42397},
	shorttitle = {Definition},
	abstract = {völkerrechtlicher Vertrag zum Schutz von Werken der Literatur und Kunst vom 9.9.1886 mit Änderungen von Paris (1896), Berlin (1908), Bern (1914), Rom (1928), Brüssel (1948), Stockholm (1967) und Paris (1971, {BGBl}. 1973 {II} 1071). Die Verwaltungsaufgaben des Verbands werden vom internationalen},
	titleaddon = {https://wirtschaftslexikon.gabler.de/definition/revidierte-berner-uebereinkunft-rbue-42397},
	type = {Text},
	author = {Meckel, Dr Astrid},
	urldate = {2025-07-24},
	langid = {german},
	note = {Publisher: Springer Fachmedien Wiesbaden {GmbH}
Section: economy},
}

@online{noauthor_jarowinkler_nodate,
	title = {Jaro–Winkler distance - Wikipedia},
	url = {https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance},
	urldate = {2025-07-24},
	file = {ED325505.pdf:/Users/romeo/Zotero/storage/HSGQG8ZW/Jaro–Winkler_distance.html:text/html},
}

@misc{mancera_pba-llm_2025,
	title = {{PBa}-{LLM}: Privacy- and Bias-aware {NLP} using Named-Entity Recognition ({NER})},
	url = {http://arxiv.org/abs/2507.02966},
	doi = {10.48550/arXiv.2507.02966},
	shorttitle = {{PBa}-{LLM}},
	abstract = {The use of Natural Language Processing ({NLP}) in highstakes {AI}-based applications has increased significantly in recent years, especially since the emergence of Large Language Models ({LLMs}). However, despite their strong performance, {LLMs} introduce important legal/ ethical concerns, particularly regarding privacy, data protection, and transparency. Due to these concerns, this work explores the use of Named- Entity Recognition ({NER}) to facilitate the privacy-preserving training (or adaptation) of {LLMs}. We propose a framework that uses {NER} technologies to anonymize sensitive information in text data, such as personal identities or geographic locations. An evaluation of the proposed privacy-preserving learning framework was conducted to measure its impact on user privacy and system performance in a particular high-stakes and sensitive setup: {AI}-based resume scoring for recruitment processes. The study involved two language models ({BERT} and {RoBERTa}) and six anonymization algorithms (based on Presidio, {FLAIR}, {BERT}, and different versions of {GPT}) applied to a database of 24,000 candidate profiles. The findings indicate that the proposed privacy preservation techniques effectively maintain system performance while playing a critical role in safeguarding candidate confidentiality, thus promoting trust in the experimented scenario. On top of the proposed privacy-preserving approach, we also experiment applying an existing approach that reduces the gender bias in {LLMs}, thus finally obtaining our proposed Privacyand Bias-aware {LLMs} ({PBa}-{LLMs}). Note that the proposed {PBa}-{LLMs} have been evaluated in a particular setup (resume scoring), but are generally applicable to any other {LLM}-based {AI} application.},
	number = {{arXiv}:2507.02966},
	publisher = {{arXiv}},
	author = {Mancera, Gonzalo and Morales, Aythami and Fierrez, Julian and Tolosana, Ruben and Penna, Alejandro and Lopez-Duran, Miguel and Jurado, Francisco and Ortigosa, Alvaro},
	urldate = {2025-07-28},
	date = {2025-07-09},
	eprinttype = {arxiv},
	eprint = {2507.02966 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/JQ9STSQS/Mancera et al. - 2025 - PBa-LLM Privacy- and Bias-aware NLP using Named-E.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/UFFYEP8T/2507.html:text/html},
}

@misc{obeidat_llms_2025,
	title = {Do {LLMs} Surpass Encoders for Biomedical {NER}?},
	url = {http://arxiv.org/abs/2504.00664},
	doi = {10.48550/arXiv.2504.00664},
	abstract = {Recognizing spans of biomedical concepts and their types (e.g., drug or gene) in free text, often called biomedical named entity recognition ({NER}), is a basic component of information extraction ({IE}) pipelines. Without a strong {NER} component, other applications, such as knowledge discovery and information retrieval, are not practical. State-of-the-art in {NER} shifted from traditional {ML} models to deep neural networks with transformer-based encoder models (e.g., {BERT}) emerging as the current standard. However, decoder models (also called large language models or {LLMs}) are gaining traction in {IE}. But {LLM}-driven {NER} often ignores positional information due to the generative nature of decoder models. Furthermore, they are computationally very expensive (both in inference time and hardware needs). Hence, it is worth exploring if they actually excel at biomedical {NER} and assess any associated trade-offs (performance vs efficiency). This is exactly what we do in this effort employing the same {BIO} entity tagging scheme (that retains positional information) using five different datasets with varying proportions of longer entities. Our results show that the {LLMs} chosen (Mistral and Llama: 8B range) often outperform best encoder models ({BERT}-(un)cased, {BiomedBERT}, and {DeBERTav}3: 300M range) by 2-8\% in F-scores except for one dataset, where they equal encoder performance. This gain is more prominent among longer entities of length {\textgreater}= 3 tokens. However, {LLMs} are one to two orders of magnitude more expensive at inference time and may need cost prohibitive hardware. Thus, when performance differences are small or real time user feedback is needed, encoder models might still be more suitable than {LLMs}.},
	number = {{arXiv}:2504.00664},
	publisher = {{arXiv}},
	author = {Obeidat, Motasem S. and Nahian, Md Sultan Al and Kavuluru, Ramakanth},
	urldate = {2025-07-28},
	date = {2025-04-01},
	eprinttype = {arxiv},
	eprint = {2504.00664 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/R4KPLMJC/Obeidat et al. - 2025 - Do LLMs Surpass Encoders for Biomedical NER.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/FHJ3AHRM/2504.html:text/html},
}

@article{cheng_novel_2024,
	title = {A novel prompting method for few-shot {NER} via {LLMs}},
	volume = {8},
	issn = {2949-7191},
	url = {https://www.sciencedirect.com/science/article/pii/S2949719124000475},
	doi = {10.1016/j.nlp.2024.100099},
	abstract = {In various natural language processing tasks, significant strides have been made by Large Language Models ({LLMs}). Researchers leverage prompt method to conduct {LLMs} in accomplishing specific tasks under few-shot conditions. However, the prevalent use of {LLMs}’ prompt methods mainly focuses on guiding generative tasks, and employing existing prompts may result in poor performance in Named Entity Recognition ({NER}) tasks. To tackle this challenge, we propose a novel prompting method for few-shot {NER}. By enhancing existing prompt methods, we devise a standardized prompts tailored for the utilization of {LLMs} in {NER} tasks. Specifically, we structure the prompts into three components: task definition, few-shot demonstration, and output format. The task definition conducts {LLMs} in performing {NER} tasks, few-shot demonstration assists {LLMs} in understanding {NER} task objectives through specific output demonstration, and output format restricts {LLMs}’ output to prevent the generation of unnecessary results. The content of these components has been specifically tailored for {NER} tasks. Moreover, for the few-shot demonstration within the prompts, we propose a selection strategy that utilizes feedback from {LLMs}’ outputs to identify more suitable few-shot demonstration as prompts. Additionally, to enhance entity recognition performance, we enrich the prompts by summarizing error examples from the output process of {LLMs} and integrating them as additional prompts.},
	pages = {100099},
	journaltitle = {Natural Language Processing Journal},
	shortjournal = {Natural Language Processing Journal},
	author = {Cheng, Qi and Chen, Liqiong and Hu, Zhixing and Tang, Juan and Xu, Qiang and Ning, Binbin},
	urldate = {2025-07-28},
	date = {2024-09-01},
	keywords = {Deep learning, Natural language processing, Large language model, Named entity recognition, Prompt method},
	file = {Cheng et al. - 2024 - A novel prompting method for few-shot NER via LLMs.pdf:/Users/romeo/Zotero/storage/JERKQAS3/Cheng et al. - 2024 - A novel prompting method for few-shot NER via LLMs.pdf:application/pdf;ScienceDirect Snapshot:/Users/romeo/Zotero/storage/MM3JY4DN/S2949719124000475.html:text/html},
}

@inbook{malbon_standards_2014,
	title = {{STANDARDS} {CONCERNING} {THE} {AVAILABILITY}, {SCOPE} {AND} {USE} {OF} {INTELLECTUAL} {PROPERTY} {RIGHTS}},
	isbn = {978-1-78100-604-7},
	url = {http://www.elgaronline.com/view/9781845424435.00026.xml},
	pages = {239--240},
	booktitle = {The {WTO} Agreement on Trade-Related Aspects of Intellectual Property Rights},
	publisher = {Edward Elgar Publishing},
	bookauthor = {Malbon, Justin and Lawson, Charles and Davison, Mark},
	urldate = {2025-08-01},
	date = {2014},
	langid = {english},
	doi = {10.4337/9781781006047.00026},
	file = {2014 - STANDARDS CONCERNING THE AVAILABILITY, SCOPE AND U.pdf:/Users/romeo/Zotero/storage/RPS2R5XT/2014 - STANDARDS CONCERNING THE AVAILABILITY, SCOPE AND U.pdf:application/pdf},
}

@online{noauthor_urhg_nodate,
	title = {{UrhG} - nichtamtliches Inhaltsverzeichnis},
	url = {https://www.gesetze-im-internet.de/urhg/},
	urldate = {2025-08-11},
	file = {UrhG - nichtamtliches Inhaltsverzeichnis:/Users/romeo/Zotero/storage/IBMNXXSJ/urhg.html:text/html},
}

@misc{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	number = {{arXiv}:2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2025-08-13},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/KWNCLRMA/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/AAC8ZQLY/2005.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2025-08-13},
	date = {2023-08-02},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/IUN46MDA/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/N5Y9BZL4/1706.html:text/html},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2025-08-13},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/T5EDTV4H/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/LGNRXN53/1810.html:text/html},
}

@article{grigoleit_natural_2019,
	title = {Natural language processing},
	url = {https://publica.fraunhofer.de/handle/publica/260578},
	doi = {10.24406/publica-fhg-260578},
	abstract = {Natural Language Processing ({NLP}) beschreibt computergestützte Techniken zur maschinellen Erkennung und Verarbeitung von natürlicher Sprache. Das Ziel ist dabei, die direkte Kommunikation zwischen Mensch und Computer auf der Basis natürlicher Sprache zu ermöglichen. {NLP}-Systeme gelten als einer der kompliziertesten Bereiche der Informatik, was daran liegt, dass die natürliche Sprache komplex und mehrdeutig ist und häufig Kontextinformationen notwendig sind, um einen Satz verstehen zu können. In den letzten Jahren wurden jedoch enorme Forschungsanstrengungen auf diesem Gebiet unternommen, so dass auch in den nächsten Jahren mit Fortschritten in den Bereichen maschinelle Übersetzung, Frage-Antwort-Systeme (wie Chatbots), Sentiment-Analyse oder automatisierte Zusammenfassungen zu rechnen ist.},
	journaltitle = {Europäische Sicherheit \& Technik : {ES} \& T},
	author = {Grigoleit, Sonja},
	urldate = {2025-08-14},
	date = {2019},
	langid = {german},
	file = {EST-0419-Natural-Language-Processing.pdf:/Users/romeo/Downloads/EST-0419-Natural-Language-Processing.pdf:application/pdf},
}


@article{lee_biobert_2020,
	title = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},
	volume = {36},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btz682},
	doi = {10.1093/bioinformatics/btz682},
	shorttitle = {{BioBERT}},
	abstract = {Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing ({NLP}), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in {NLP} to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model {BERT} can be adapted for biomedical corpora.We introduce {BioBERT} (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, {BioBERT} largely outperforms {BERT} and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While {BERT} obtains performance comparable to that of previous state-of-the-art models, {BioBERT} significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% {MRR} improvement). Our analysis results show that pre-training {BERT} on biomedical corpora helps it to understand complex biomedical texts.We make the pre-trained weights of {BioBERT} freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning {BioBERT} available at https://github.com/dmis-lab/biobert.},
	pages = {1234--1240},
	number = {4},
	journaltitle = {Bioinformatics},
	shortjournal = {Bioinformatics},
	author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	urldate = {2025-08-14},
	date = {2020-02-15},
	file = {Full Text PDF:/Users/romeo/Zotero/storage/3LK7F5BT/Lee et al. - 2020 - BioBERT a pre-trained biomedical language represe.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/CBF56SJH/5566506.html:text/html},
}

@misc{niu_osint_2025,
	title = {{OSINT} or {BULLSHINT}? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War},
	url = {http://arxiv.org/abs/2508.03599},
	doi = {10.48550/arXiv.2508.03599},
	shorttitle = {{OSINT} or {BULLSHINT}?},
	abstract = {This paper examines the role of Open Source Intelligence ({OSINT}) on Twitter regarding the Russo-Ukrainian war, distinguishing between genuine {OSINT} and deceptive misinformation efforts, termed "{BULLSHINT}." Utilizing a dataset spanning from January 2022 to July 2023, we analyze nearly 2 million tweets from approximately 1,040 users involved in discussing real-time military engagements, strategic analyses, and misinformation related to the conflict. Using sentiment analysis, partisanship detection, misinformation identification, and Named Entity Recognition ({NER}), we uncover communicative patterns and dissemination strategies within the {OSINT} community. Significant findings reveal a predominant negative sentiment influenced by war events, a nuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the potential strategic manipulation of information. Additionally, we apply community detection techniques, which are able to identify distinct clusters partisanship, topics, and misinformation, highlighting the complex dynamics of information spread on social media. This research contributes to the understanding of digital warfare and misinformation dynamics, offering insights into the operationalization of {OSINT} in geopolitical conflicts.},
	number = {{arXiv}:2508.03599},
	publisher = {{arXiv}},
	author = {Niu, Johannes and Stillman, Mila and Kruspe, Anna},
	urldate = {2025-08-14},
	date = {2025-08-05},
	eprinttype = {arxiv},
	eprint = {2508.03599 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Social and Information Networks},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/WVRMFE2L/Niu et al. - 2025 - OSINT or BULLSHINT Exploring Open-Source Intellig.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/G2NWVK5J/2508.html:text/html},
}

@book{chan_generative_2024,
	location = {Abingdon, Oxon},
	title = {Generative {AI} in Higher Education : The {ChatGPT} Effect},
	isbn = {978-1-03-259904-5},
	url = {https://research.ebsco.com/linkprocessor/plink?id=dd1cd7e9-07cf-3165-9c35-39e8d4b3faaa},
	shorttitle = {Generative {AI} in Higher Education},
	abstract = {Chan and Colloton's book is one of the first to provide a comprehensive examination of the use and impact of {ChatGPT} and Generative {AI} ({GenAI}) in higher education.Since November 2022, every conversation in higher education has involved {ChatGPT} and its impact on all aspects of teaching and learning. The book explores the necessity of {AI} literacy tailored to professional contexts, assess the strengths and weaknesses of incorporating {ChatGPT} in curriculum design, and delve into the transformation of assessment methods in the {GenAI} era. The authors introduce the Six Assessment Redesign Pivotal Strategies ({SARPS}) and an {AI} Assessment Integration Framework, encouraging a learner-centric assessment model. The necessity for well-crafted {AI} educational policies is explored, as well as a blueprint for policy formulation in academic institutions. Technical enthusiasts are catered to with a deep dive into the mechanics behind {GenAI}, from the history of neural networks to the latest advances and applications of {GenAI} technologies.With an eye on the future of {AI} in education, this book will appeal to educators, students and scholars interested in the wider societal implications and the transformative role of {GenAI} in pedagogy and research.The Open Access version of this book, available at www.taylorfrancis.com, has been made available under a Creative Commons Attribution-Non Commercial-No Derivatives ({CC}-{BY}-{NC}-{ND}) 4.0 license.},
	publisher = {Routledge},
	author = {Chan, Cecilia Ka Yuk and Colloton, Tom},
	urldate = {2025-01-05},
	date = {2024-01-01},
	keywords = {Artificial intelligence--Educational applications, {COMPUTERS} / Data Science / Machine Learning, {EDUCATION} / Administration / Higher, {EDUCATION} / Computers \& Technology, {EDUCATION} / General, Education, Higher--Effect of technological innovations on},
	file = {Chan und Colloton - 2024 - Generative AI in Higher Education  The ChatGPT Ef.pdf:/Users/romeo/Zotero/storage/9HRBNQEL/Chan und Colloton - 2024 - Generative AI in Higher Education  The ChatGPT Ef.pdf:application/pdf},
}

@misc{hu_lora_2021,
	title = {{LoRA}: Low-Rank Adaptation of Large Language Models},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	shorttitle = {{LoRA}},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using {GPT}-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or {LoRA}, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to {GPT}-3 175B fine-tuned with Adam, {LoRA} can reduce the number of trainable parameters by 10,000 times and the {GPU} memory requirement by 3 times. {LoRA} performs on-par or better than fine-tuning in model quality on {RoBERTa}, {DeBERTa}, {GPT}-2, and {GPT}-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of {LoRA}. We release a package that facilitates the integration of {LoRA} with {PyTorch} models and provide our implementations and model checkpoints for {RoBERTa}, {DeBERTa}, and {GPT}-2 at https://github.com/microsoft/{LoRA}.},
	number = {{arXiv}:2106.09685},
	publisher = {{arXiv}},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	urldate = {2025-08-14},
	date = {2021-10-16},
	eprinttype = {arxiv},
	eprint = {2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/PJKXNPZD/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/V4NY7SG2/2106.html:text/html},
}

@misc{xu_survey_2024,
	title = {A Survey on Knowledge Distillation of Large Language Models},
	url = {http://arxiv.org/abs/2402.13116},
	doi = {10.48550/arXiv.2402.13116},
	abstract = {In the era of Large Language Models ({LLMs}), Knowledge Distillation ({KD}) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary {LLMs}, such as {GPT}-4, to their open-source counterparts like {LLaMA} and Mistral. Additionally, as open-source {LLMs} flourish, {KD} plays a crucial role in both compressing these models, and facilitating their self-improvement by employing themselves as teachers. This paper presents a comprehensive survey of {KD}'s role within the realm of {LLM}, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self-improvement. Our survey is meticulously structured around three foundational pillars: {\textbackslash}textit\{algorithm\}, {\textbackslash}textit\{skill\}, and {\textbackslash}textit\{verticalization\} -- providing a comprehensive examination of {KD} mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation ({DA}) and {KD}, illustrating how {DA} emerges as a powerful paradigm within the {KD} framework to bolster {LLMs}' performance. By leveraging {DA} to generate context-rich, skill-specific training data, {KD} transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in {KD} and proposing future research directions. Importantly, we firmly advocate for compliance with the legal terms that regulate the use of {LLMs}, ensuring ethical and lawful application of {KD} of {LLMs}. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-{LLMs}.},
	number = {{arXiv}:2402.13116},
	publisher = {{arXiv}},
	author = {Xu, Xiaohan and Li, Ming and Tao, Chongyang and Shen, Tao and Cheng, Reynold and Li, Jinyang and Xu, Can and Tao, Dacheng and Zhou, Tianyi},
	urldate = {2025-08-14},
	date = {2024-10-21},
	eprinttype = {arxiv},
	eprint = {2402.13116 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/KFS2TB5N/Xu et al. - 2024 - A Survey on Knowledge Distillation of Large Langua.pdf:application/pdf},
}

@misc{egashira_exploiting_2024,
	title = {Exploiting {LLM} Quantization},
	url = {http://arxiv.org/abs/2405.18137},
	doi = {10.48550/arXiv.2405.18137},
	abstract = {Quantization leverages lower-precision weights to reduce the memory usage of large language models ({LLMs}) and is a key technique for enabling their deployment on commodity hardware. While {LLM} quantization's impact on utility has been extensively explored, this work for the first time studies its adverse effects from a security perspective. We reveal that widely used quantization methods can be exploited to produce a harmful quantized {LLM}, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model. We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious {LLM} through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an {LLM} that exhibits benign behavior in full precision but when quantized, it follows the adversarial behavior injected in step (i). We experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack. In practice, the adversary could host the resulting full-precision model on an {LLM} community hub such as Hugging Face, exposing millions of users to the threat of deploying its malicious quantized version on their devices.},
	number = {{arXiv}:2405.18137},
	publisher = {{arXiv}},
	author = {Egashira, Kazuki and Vero, Mark and Staab, Robin and He, Jingxuan and Vechev, Martin},
	urldate = {2025-08-14},
	date = {2024-11-04},
	eprinttype = {arxiv},
	eprint = {2405.18137 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/8IHLKPG9/Egashira et al. - 2024 - Exploiting LLM Quantization.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/6ULW3MQ5/2405.html:text/html},
}

@misc{zhang_sinklora_2024,
	title = {{SinkLoRA}: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models},
	url = {http://arxiv.org/abs/2406.05678},
	doi = {10.48550/arXiv.2406.05678},
	shorttitle = {{SinkLoRA}},
	abstract = {Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. {LongLoRA} proposed shifted sparse attention (S{\textbackslash}({\textasciicircum}2{\textbackslash})-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, {LongLoRA} is still not as efficient as vanilla attention, reaching only 39{\textbackslash}\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose {\textbackslash}textbf\{{SinkLoRA}\}, which features better work partitioning. Specifically, (1) we developed {SF}-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of "sink attention tokens", achieving 92{\textbackslash}\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a {SOTA} {KV} cache compression algorithm H\$\_2\$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with {SinkLoRA} using a self collected {LongAlpaca}-plus dataset. All our code, models, datasets, and demos are available at {\textbackslash}url\{https://github.com/Dexter-{GT}-86/{SinkLoRA}\}.},
	number = {{arXiv}:2406.05678},
	publisher = {{arXiv}},
	author = {Zhang, Hengyu},
	urldate = {2025-08-17},
	date = {2024-06-09},
	eprinttype = {arxiv},
	eprint = {2406.05678 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/romeo/Zotero/storage/YV2WJ46R/Zhang - 2024 - SinkLoRA Enhanced Efficiency and Chat Capabilitie.pdf:application/pdf;Snapshot:/Users/romeo/Zotero/storage/6YIMZFUZ/2406.html:text/html},
}